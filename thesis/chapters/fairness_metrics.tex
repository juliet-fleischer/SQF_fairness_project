\section*{Definitions of Fairness in Machine Learning}
When one starts to get into the topic of fairness in machine learning, it is easy to get overwhelmed by the sheer amount of definitions and metrics that are out there. In this chapter we try to group them in an intuitive way and motivate them in the hope to bring some clarity to readers. It is helpful to make one or both of the two classifications. We can distinguish (a) between observational vs. causality-based criteria; or (b) group vs. individual criteria \cite{castelnovo2022}. In this paper we will combine these two, as a fairness metric is always in one of (a) and (b) e.g. a group metric is also an observational metric. This will become clearer in the following.

Broadly speaking, group fairness aims to create equality between groups and individual fairness aims to create equality between two individuals within a group. Observational fairness metrics act descriptive and use the observed distribution of the data to assess fairness while causality-based criteria make assumptions about the causal structure of the data and base their notion of fairness on this (so basically observational says, fairness is when I can measure equality from my distribution and causality says fairness is when the cause for my decision is not discriminatory against someone or a group). On the basis of these fundamental ideas, a plethora of formalisations have emerged. Most of them concern themselves with defining fairness for a binary classification task and one binary protected attribute (PA). The extension to a multiclass PA is the easiest. Multiple sensitive attributes bring challenges with them that are not as straightforward to address. Also, the extension from binary classification to other tasks, such as neural networks, LLMs is subject to ongoing research. As this work is meant to help you start thinking about fairness in machine learning, we will limit ourselves to the binary classification case.
Specifically, we want to use the following running example, inspired by our case study on real data in chapter x. The crime rates in NYC should be decreased with the help of a new AI tool. Specifically, the administration orders a team of machine learning experts to design an automated decision-making system that should predict criminal activity of a person. It should be employed by police officers to decide whether to stop a person or investigate them further. Past police stops serve as training data. Given the history of racial profiling in the United States, it is reasonable to raise concerns about racial decision patterns the algorithm could learn from. First, we approach this from a group fairness perspective.

\subsection*{Group fairness}
The notion of fairness underlying group metrics is that discrimination of certain groups of the population defined via the protected attribute should be prevented. Group fairness can be grouped into three main categories, independence, separation, and sufficiency. 

\textbf{Independence} is in a sense the simplest group fairness metric. It requires that the prediction $\hat{Y}$ is independent of the protected attribute $A$, so $\hat{Y} \perp A$. In other words, the positive prediction ratio (ppr) should be the same for all values of $A$. For a binary classification task with binary sensitive attribute this can be formalised as demographic parity/statistical parity $P(\hat{Y} | A = a) = P(\hat{Y} | A = b)$.
The other two groups of group fairness metrics, Separation and Sufficiency can both be derived from the error matrix.
\textbf{Separation} requires independence between $\hat{Y}$ and $A$ conditioned on the true label $Y$, so $\hat{Y} \perp A | Y$. This means that the focus is on equal error rates between groups, which gives rise to the following list of fairness metrics:
\begin{itemize}
    \item Equal opportunity/ False negative error rate balance: $P(\hat{Y} = 0 | Y = 1, A = a) = P(\hat{Y} = 0 | Y = 1, A = b)$ or $P(\hat{Y} = 1 | Y = 1, A = a) = P(\hat{Y} = 1 | Y = 1, A = b)$ \texttt{mlr3}: \texttt{fairness.fnr}, \texttt{fairness.tpr}
    \item Predictive equality/ False positive error rate balance: $P(\hat{Y} = 1 | Y = 0, A = a) = P(\hat{Y} = 1 | Y = 0, A = b)$ or \\ $P(\hat{Y} = 0 | Y = 0, A = a) = P(\hat{Y} = 0 | Y = 0, A = b)$ \texttt{mlr3}: \texttt{fairness.fpr}, \texttt{fairness.tnr}
    \item Equalized odds: $P(\hat{Y} = 1 | Y = y, A = a) = P(\hat{Y} = 1 | Y = y, A = b) \forall y \in \{0, 1\}$ \\ \texttt{mlr3}: \texttt{fairness.equalized.odds}
    \item Overall accuracy equality: $P(\hat{Y} = Y | A = a) = P(\hat{Y} = Y | A = b)$ \\ \texttt{mlr3}: \texttt{fairness.acc}
    \item Treatment equality: $\frac{\text{FN}}{\text{FP}} \big|_{A = a} = \frac{\text{FN}}{\text{FP}} \big|_{A = b}$
\end{itemize}

Equal opportunity requires the false negative rates, the ratio of actual positive people that were wrongly predicted as negative, to be equal between groups. Therefore, it is also called false negative error rate balance. Since equal true positive rates between groups are simultaneously fulfilled, one could also define equal opportunity via the true positive rate.
Predictive equality follows the same principle as equal opportunity but instead of focusing on the false negatives, it focuses on the false positives. Again, if a classifier has equal false positive rates between groups, it also has equal true negative rates. With its focus on the false positive rates, predictive equality is also presented in the context of punitive tasks. Since people could experience potential harm on the basis of a positive prediction, the proportion of truly innocent people that do not deserve punishment should be kept at a minimum. For assistive tasks, such as deciding who receives some kind of welfare, a focus on minimising the false negative rate could be more relevant. 
Equalized odds combines equal opportunity and predictive equality. It requires that the false positive and true positive rates are equal between groups, and is in this sense stricter than either of them alone. Treatment Equality is another variation that forms the error ratio for each group and requires it to be equal. Finally, overall accuracy equality simply requires equal accuracy between groups, meaning equal proportion of correctly classified individuals in each group.
\textbf{Sufficiency} requires independence between $Y$ and $A$ conditioned on $\hat{Y}$, so $Y \perp A | \hat{Y}$. Intuitively this means that we want a prediction to be equally credible between groups. Instead of conditioning vertically on the true labels, we now condition horizontally on the predictions. This leads to the following fairness metrics:

\begin{itemize}
    \item Predictive parity/ outcome test: $P(Y = 1 | \hat{Y} = 1, A = a) = P(Y = 1 | \hat{Y} = 1, A = b)$ \texttt{mlr3}: \texttt{fairness.ppv}
    \item Equal true negative rate: $P(Y = 0 | \hat{Y} = 0, A = a) = P(Y = 0 | \hat{Y} = 0, A = b)$ \texttt{mlr3}: \texttt{fairness.npv}
    \item Equal false omission rate: $P(Y = 1 | \hat{Y} = 0, A = a) = P(Y = 1 | \hat{Y} = 0, A = b)$ \texttt{mlr3}: \texttt{fairness.fomr}
    \item Equal false discovery rate: $P(Y = 0 | \hat{Y} = 1, A = a) = P(Y = 0 | \hat{Y} = 1, A = b)$ 
    \item Conditional use accuracy equality: $P(Y = 1 | \hat{Y} = 1, A = a) = P(Y = 1 | \hat{Y} = 1, A = b) \land P(Y = 0 | \hat{Y} = 0, A = a) = P(Y = 0 | \hat{Y} = 0, A = b)$
\end{itemize}

Predictive parity requires that the probability of actually being positive, given a positive prediction is the same between groups. Following the same principle, we can require that the probability of actually being negative, given a negative prediction is the same between groups. If we instead look at errors again, we can require equal false omission rates between groups or equal false discovery rates between groups. False omission describes the case in which an actual positive person is predicted as negative and can be highly relevant in assistive settings, such as description of a medical treatment. False discovery rate describes the case in which an actual negative person is predicted as positive. This should be taken into account in punitive settings, in which we do not want to convict innocent people. By not only requiring one of these criteria but two simultaneously, we can build a stronger metric, like conditional use accuracy equality that requires same positive predictive values between groups and same negative predictive values between groups.
Hopefully, the pattern becomes clear now. While it is easy to get overwhelmed by the amount of definitions at first, taking a closer look, it becomes clear that they are constructed in a structured way. In fact, equal false omission rate and equal false discovery rate were not introduced in the paper \cite{verma2018} but it is clear that they follow the same pattern as the other metrics.

Most (binary) classifiers work with predictions scores and a hard label classifier is applied only afterwards in form of a treshold criterion. It should therefore come as no surprise that instead of formulating fairness with $\hat{Y}$ there exist fairness metrics that use the score $S$, which typically represents the probability of belonging to the positive class.
\begin{itemize}
    \item Calibration: $P(Y = 1 | S = s, A = a) = P(Y = 1 | S = s, A = b)$
    \item Well-calibration: $P(Y = 1 | S = s, A = a) = P(Y = 1 | S = s, A = b) = s$
    \item Balance for positive class: $E(S \mid Y = 1, A = a) = E(S \mid Y = 1, A = b)$
    \item Balance for negative class: $E(S \mid Y = 0, A = a) = E(S \mid Y = 0, A = b)$
\end{itemize}
Calibration requires that the probability for actually being positive, given a score $s$ is the same between groups. As the score can usually take values from the whole real number line, this can in practive be implmented by binning the scores \cite{verma2018}. Well-calibration is a stronger version of this, requiring that the probability for actually being positive, given a score $s$ is the same between groups and equal to the score itself. This means, when for a set of suspects the classifier predicts a certain probability $s$ of crime, then the proportion of people that actually committed crime should be $s$. Balance for the positive class takes the expectation over the predictions scores of the people that are actually positive and wants them to be equal across groups. We do not want that one the positive people of one group get on average a higher score than the positive people of another group. The same holds for the negative class, formalized as balance for the negative class.
To constrast the group fairness criteria, sufficiency takes the perspective of the decision-making instance, as usually only the prediction is known to them in the moment of decision. For example, the police, who do not yet know the true label at the time when they are supposed to decide whether someone would become a criminal.
As separation criteria condition on the true label Y it is suitable when we can be sure that $Y$ is free from any bias, so to say when $Y$ was generated via an objectively true process (this will become clearer in the chapter on bias).
Independence is best, when we want to enfore a form of equality between groups, regardless of context or any potential personal merit. While this seems be useful in cases in which the data contains complex bias, it is unclear whether this enforcments has the intended benefits, especially over the long term. {\color{red}{reference?}}

\subsection*{Individual fairness}
By creating equality between groups, it can happen that individual people are being treated unfairly. If we want to equalise e.g. the false positive rates between two groups and currently group a has a higher false positive rate than group b, this would lead us to lowering the prediction threshold for b, such that more actual negative people would get classified as positive. Or it we would need to set a higher thresholkd for group a, such that it becomes harder for them to be classified as positive. Depending on the context, either option can seem unfair. Individual metrics therefore shift the focus. The underlying idea of fairness is that similar individuals should be treated in a similar way. Different individuals should be treated in a different way. It is an intuitive idea that was already formulate by greek philosopher {\color{red}{bothmann citation}}.
\textbf{Fairness through awareness (FTA)} formalizes this idea as Lipschitz criterion. $$d_Y(\hat{y_i}, \hat{y_j}) \leq \lambda {d_X}(x_i, x_j)$$
This simply puts an upper bound to the distance between predictions of two individuals, which depends on the features of them. In other words, if two people are similar in their features, they should also get similar predictions from the algorithm. The challenge of FTA is the definition of precisely this equality in the feature space. Defining when two individuals are similar is not much different from defining fairness in the first place \cite{castelnovo2022}. 
Threre is no clear solution to this. In any case, the choice of $d_X$ should take context-specific information into account. We want to find a distance metric, that suits the target and represents an ethical formalisation of similarity in the features.

\textbf{Fairness through unawareness (FTU) or blinding}
This is primarly formulated as procedural rule. Blinding tells us to not use the protected attribute explicitly in the decision-making process. So at first this would simply mean to discard the protected attribute from the data during training. After training, FTU can be tested by simulating a doppelganger for each person in the dataset. This doppelganger has the exact same features with the excpetion of the protected attribute, which is flipped (easy in binary PA case). If both these instances have the same prediction, the algorithm would satisfie FTU \cite{verma2018}. This is actually also a from of FTA, in which we chose the distance metric to measure a distance of zero only if two people are the same on all their features except for the protected attribute. Blinding is however to be seen critically.
FTU has the problem of proxies. These are variables that are strongly correlated with the protected attribute. Therefore, it's not enough to simply mask the information of the sensitive attribute druing training because discrimination can persist via these proxis. This becomes clearer, when imagine that we remove information, such that this feature is simply not available to the classifier during training. The place of residence, however, is strongly correlated with the person's ethnicity. Thus, indirect discrimination based on ethnicity remains, even though the information was not directly available during training. Suppression therefore extends the idea of blinding and the goal is to develope a model that is blind to the sensitive attribute and the proxis. The drawback is, that it is unclear when a feature is sufficiently correlated with the sensitive attribute to be counted as proxi. Additionally, we could loose important information by removing too many these features \cite{castelnovo2022}.

\subsection*{Causality-based notions}
The previously discussed notions of fairness can be counted a observational. In contrast to them, causality-based notions ask whether the sensitive attribute was the \textit{reason} for the decision. If a certain (harmful) decision was made \textit{because of} the value of the sensitive attribute of a person, we deem the algorithm as unfair.
There are causality-based concepts that focus on group-level fairness and also some that focus on individual-level fairness. We want to give an intoduction to all of them, but since this category requires a new theory we will not get into great detail.

How do we make it fairer now? In principle, there are three approaches, which can be categorised as preprocessing, inprocessing or postprocessing. Depending on when they take place in the machine learning pipeline. Preprocessing methods have the idea that the data should be processed before training, so that our algorithm learns, for example, practically on corrected data and thus no discrimination can take place. This can be achieved by sampling or transformations, for example. In Processing, we have methods for which we need to have access to the algorithm, because the approach here is that we really modify the optimisation problem itself by actually taking the loss function and, for example, appending a regularisation term and thus optimising our algorithm not only for high accuracy, but also for one of the fairness metrics mentioned above. Postprocessing now works again with black box algorithms, just like preprocessing, because here we simply take the finished labels that our algorithm has predicted and also set the scores and, for example, individual group thresholds in order to then do justice to predictive equality or whatever. So here again, there really are a wide variety of methods, some adapted to specific algorithms, some more universal. With all these methods and all these definitions of fairness, it's really easy to get lost, especially when you're applying the whole thing. It is therefore very important. But when it comes to fairness, it's very important that you don't get bogged down in the details, that you always take a step back and look at your data and your algorithm in a wider context.

You can think of the whole thing as a kind of cycle, also known as a feedback loop, which consists of personal data and an algorithm. Because the fact is that our algorithm learns from data, but of course this data practically reflects our reality, i.e. our society to a certain extent, and we then make decisions based on this trained algorithm. And so we practically have a kind of feedback loop, a kind of cycle, at each of these three points bias can be introduced into the process and, above all, bias can also be reinforced in the course of this process. So, for example, let's take a closer look at the whole thing. So let's look at the whole thing again. Let's illustrate the whole thing again using our example from before. So it's like this now, so we can imagine that in the past the police have experienced a lot of discrimination, a lot of discrimination towards the police, towards people of colour, and therefore, for example, people of colour are stopped much more often by the police, much more police presence simply takes place in certain regions where people of colour live and we therefore record much higher crime rates among people of colour. And that is then reflected. And so we, as people, as a society, make practically discriminatory decisions, whereby it can also be that bias is introduced into this cycle through the data, for example, in which our data is sampled through a distorted process and does not actually reflect our basic population, our true population, i.e. we have a disproportionate number of white people in our data set and therefore the faces of white people are then later better recognised than those of black people, simply because our sample does not actually reflect the world population.

Or the algorithm simply uses biased estimates and therefore introduces discrimination into the cycle even with correct data and no historical bias. In principle, we have now illustrated the topic of fair machine learning using the simplest case of a binary classification and a protected attribute. Of course, in reality it is often more complex. People don't just belong. We have several protected attributes People belong to different groups and we naturally have more diverse tasks such as regressions and supervised learning, where we also want to ensure fairness. So there are many more in research, so there are many more extensions of this. And in research, there are many more extensions to the current definitions of fairness. And there is also the fact that we always have to take our context and the data into account. And that is why there is not yet a single definition of fairness. And it is always important to keep the specific context, the task and the data in mind. I hope that I have been able to arouse your curiosity about the topic and thank you for your attention.




