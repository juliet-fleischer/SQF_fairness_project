
I would like to start my talk on fair machine learning today with an example. Let's imagine that we, as data scientists, have been tasked with helping the New York Police Department bring down crime rates. And we, as data scientists, help ourselves by wanting to train a machine learning algorithm that predicts whether a person would commit a criminal offence. And the New York Police Department provides us with past police stops as training data. Now there's a problem that can arise: the past police stops are based on the police stops. In the past, of course, past police stops were based on past police decisions and thus also on possible discrimination associated with them, such as racial profiling. Our algorithm will. These structures will then learn very well to reproduce these structures and thus possible racial discrimination could be brought into our algorithm. So how can we as data scientists now ensure that our algorithm is fair? This is exactly what the research field of fair machine learning is concerned with and I have recently gained an overview of it and would now like to give you the most important things to take away with you. I will therefore start my presentation by introducing you to how we actually think about fairness in machine learning. This will also be the main focus of my presentation, because we will realise that it is not that simple. I will then go into a few more fairness methods and then make a somewhat critical comment by talking about bias and the feedback loop. In fair machine learning, there are now basically two basic ideas for thinking about fairness.

Group fairness and individual fairness. Group fairness aims to create equality between groups and individual fairness aims to create equality between two individuals within a group. And on the basis of these two fundamental ideas, a really wide variety of formalisations have now emerged, not all of which we will go into in detail, but I would rather give you the most important subgroups and, above all, the intuition behind them. And if I ever go into a really explicit definition, I will use an English term, simply to avoid confusion. You'll find more details and an overview of really precise definitions in the handout that I'll give you later. We will now take the STANDARD. That means us. We'll keep our example in mind for all of this. We want to train a binary classification algorithm that predicts whether the person will commit an offence yes or no? And we are particularly concerned about racial discrimination. So now we can go and say okay, we'll share. Let's assume, just to keep it simple here, that we differentiate between white people and people of colour, i.e. practically people with a migration background. And we want to make sure and can now approach this with this idea of group fairness and say We don't want people to experience discrimination because of their group affiliation and therefore demand independence between the prediction and the group affiliation. This is called independence and is one of the three large subgroups of the Fairness and Fair Machine Learning groups.

The group membership, labelled here as variable a, is now defined via the so-called protected attribute. In fair machine learning, a protected attribute is simply a characteristic that should not be used to discriminate. For example, ethnicity. And we now want the same prediction rates to exist between both white people and people of colour. We can visualise this very easily by simply saying that we now simply assume we have 15 white people and 15 people of colour, on the basis of which we define our data set, on the basis of which we now define fairness. And we want both 3/15 white people and 3/15 people with a migration background to be predicted as criminals. Are predicted as criminal. Then we say our algorithm is fair. For example, we call this statistical parity and demand equal probability for epsilon one between our two groups. Now another subgroup of groups. Fairness is separation, which also requires independence between Y, HUD and A, but then conditional on the true label and thus separation focusses on equal error rates between the groups. So, for example, if we see here now that among the whites we have wrongly predicted two as criminal. And in the same way, we also want two people to be wrongly predicted as criminals among people of colour. This may mean that we have to set different thresholds depending on the group. Which may seem strange to us intuitively at first. Not in terms of intuitive fairness, but as a consequence.

We can calculate accordingly. So here, for example, we demand the same false positive straight, but we can actually derive separation and also sufficiency, the third large group fairness metric, simply by using the error matrix as a vertical condition and can not only demand the same false positive rate, but also the same if negative rate, true, positive rate and so on. And predictive parity formalises this, for example, by demanding the same false positive freedom. The same probability for Y equal to one, if one is actually zero, should be equal between both groups and to fish. It can be constructed in exactly the same way using the error matrix. However, by now conditioning horizontally on the prediction here, we practically want the reliability of a prediction to be the same. Between the two groups, for example, predictive parity requires that the probability for Y is equal to one. One was predicted as one, so give one was predicted as criminal. How likely is it then that you really are a criminal? This should be the same between both groups. So both separation and sufficiency can now be formulated with es instead of Y hud, if we have that. It is simply our score. We usually have it, for example in the logistic model. It's not that we only get the prediction one or zero, but we get the probability of belonging to the positive group. In this case, that would be S and so we can, for example, demand calibration and instead of fishing on Y hat conditions as in to condition them on S and demand equality between the groups here.

Or well. Calibration is a variation of Kalibration, which is simply a little stronger and also refers to the S. To summarise, we can say that sufficiency now takes on the perspective of the person making the decision. For example, the police, who do not yet know the true label at the time when they are supposed to decide whether someone would become a criminal and therefore do not yet know the true label. Separation is When it comes to separation, we often condition the true Y and therefore separation is particularly good if we can be sure that this true Y has been created through an objective, true process and does not carry any bias, which could perhaps be a bit critical in our case, which I will come back to later. And independence is especially good if a form of equality between groups is to be enforced. So no matter what. The idea is always to predict 50% as criminal . By us. The group metrics are. In a way, the simplest metrics. They are easy to understand using the error matrix, which is normally familiar to every statistician. And so the rough metrics are also implemented in well-known software packages, such as Matrix Fairness, where they are often not published individually. In other words, not the individual true positive trades between the groups, but as differences.

And these should then be zero. We want zero or as a quotient. We want to achieve one thing. They are simple, practical. However, by wanting to enforce or create equality between groups in a certain way, it can lead to individual people being treated unfairly. We have already seen this to a certain extent in the diagram. We have to set different thresholds and so, somehow, individual people who are actually negative are now predicted as positive. Simply because we want to have equal false positive rates between our groups. Independence is now. So in a way, there are those who are saying the individual fairness metrics now. Fairness to me is when two similar individuals are treated similarly. Fairness through awareness fta basically formalises it by requiring a Lipschitz criterion. And this does nothing other than simply limiting the distance between the prediction of two individuals upwards, depending on our features. So if two people are very similar, if they were somehow stopped by the police for the same reason, if they have the same criminal history, if they both own a gun but then belong to a different ethnicity. One is white, the other is black, then we don't want those two to get a different prediction. Otherwise we would consider the algorithm unfair. The difficulty that FDA brings with it is the definition of precisely this equality, i.e. the distance measure in the feature space.

When do we say that two individuals are the same? Is it enough if they live in the same place? Or does it make more sense to also consider the criminal history? This is not clear-cut and should in each case be adapted by FDA to the specific situation and to the specific protected attributes, depending on what makes the most sense in this context. And so FDA is in some ways contrary to the second major individual fairness approach FTu fairness through own awareness or also known as blinding, because here we basically want the protected attribute to simply play no role at all in our entire decision-making process and also in how we design this algorithm now. So it is more of a procedural rule than really a mathematical definition that needs to be fulfilled, in which FTU tells us that the protected attribute should not be used in the decision-making process. So, for example, if we were to train a logistic regression or a random forest, we would delete the person's ethnicity from our training data or hide it somehow during training so that our classification algorithm cannot access it at all. Ftu has the problem of proxies. These are variables that are strongly correlated with the protected attribute. Normally, it's not enough to simply say I'm going to delete the ethnicity from my data set, because then my algorithm can't make a decision based on it, because it's possible that the place of residence is extremely strongly correlated with the person's ethnicity. And so indirect discrimination based on ethnicity practically remains, even though we have deleted the ethnicity from our data set.

Then there are approaches that say well, then you just delete all the proxies from the data set, but that then has other problems, like the one there, that our algorithm then gets worse and worse, that at some point we simply delete a lot of variables and all of that is very ambiguous and is not easy to solve. In a way, this fact that we often have complex structures in our data, complex relationships. Now taken up by the third major category of fairness definitions. These are the causal definitions of fairness. These definitions are identified in the literature as the third broad category, which is appropriate. There are both causal group definitions of fairness and causal individual definitions of fairness. But they are once again a third major category in that they really try, in principle, to understand and model these dependency structures in the data. And the individual and group fairness metrics presented so far do not aim to do this; they are more observational and descriptive in the sense that we are not making any assumptions about the data. I can't go any deeper into the causal methods at this point, because they are simply very extensive. We will now focus more on the fairness methods. So what happens if we have established that our algorithm does not fulfil any of these selected fairness methods?

How do we make it fairer now? In principle, there are three approaches, which can be categorised as preprocessing, inprocessing or postprocessing. Depending on when they take place in the machine learning pipeline. Preprocessing methods have the idea that the data should be processed before training, so that our algorithm learns, for example, practically on corrected data and thus no discrimination can take place. This can be achieved by sampling or transformations, for example. In Processing, we have methods for which we need to have access to the algorithm, because the approach here is that we really modify the optimisation problem itself by actually taking the loss function and, for example, appending a regularisation term and thus optimising our algorithm not only for high accuracy, but also for one of the fairness metrics mentioned above. Postprocessing now works again with black box algorithms, just like preprocessing, because here we simply take the finished labels that our algorithm has predicted and also set the scores and, for example, individual group thresholds in order to then do justice to predictive equality or whatever. So here again, there really are a wide variety of methods, some adapted to specific algorithms, some more universal. With all these methods and all these definitions of fairness, it's really easy to get lost, especially when you're applying the whole thing. It is therefore very important. But when it comes to fairness, it's very important that you don't get bogged down in the details, that you always take a step back and look at your data and your algorithm in a wider context.

You can think of the whole thing as a kind of cycle, also known as a feedback loop, which consists of personal data and an algorithm. Because the fact is that our algorithm learns from data, but of course this data practically reflects our reality, i.e. our society to a certain extent, and we then make decisions based on this trained algorithm. And so we practically have a kind of feedback loop, a kind of cycle, at each of these three points bias can be introduced into the process and, above all, bias can also be reinforced in the course of this process. So, for example, let's take a closer look at the whole thing. So let's look at the whole thing again. Let's illustrate the whole thing again using our example from before. So it's like this now, so we can imagine that in the past the police have experienced a lot of discrimination, a lot of discrimination towards the police, towards people of colour, and therefore, for example, people of colour are stopped much more often by the police, much more police presence simply takes place in certain regions where people of colour live and we therefore record much higher crime rates among people of colour. And that is then reflected. And so we, as people, as a society, make practically discriminatory decisions, whereby it can also be that bias is introduced into this cycle through the data, for example, in which our data is sampled through a distorted process and does not actually reflect our basic population, our true population, i.e. we have a disproportionate number of white people in our data set and therefore the faces of white people are then later better recognised than those of black people, simply because our sample does not actually reflect the world population.

Or the algorithm simply uses biased estimates and therefore introduces discrimination into the cycle even with correct data and no historical bias. In principle, we have now illustrated the topic of fair machine learning using the simplest case of a binary classification and a protected attribute. Of course, in reality it is often more complex. People don't just belong. We have several protected attributes People belong to different groups and we naturally have more diverse tasks such as regressions and supervised learning, where we also want to ensure fairness. So there are many more in research, so there are many more extensions of this. And in research, there are many more extensions to the current definitions of fairness. And there is also the fact that we always have to take our context and the data into account. And that is why there is not yet a single definition of fairness. And it is always important to keep the specific context, the task and the data in mind. I hope that I have been able to arouse your curiosity about the topic and thank you for your attention.




