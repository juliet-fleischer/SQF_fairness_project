% \subsection*{Fairness Experiment: Stop, Question, and Frisk data}
After introducing the theoretical tools for assessing fairness, we turn to a case study on the stop-and-frisk practice. A police officer is allowed to stop a person if they have reasonable suspicion that the person has committed, is committing, or is about to commit a crime.
During the stop the officer is allowed to frisk a person (pat-down the person's outer clothing) or search them more carefully.
The stop can result in a summon, an arrest or no further consequences. After a stop was made, the officer is required to fill out a form, documenting the stop. This data is published yearly by the NYPD.
As mentioned in the introduction the so-called "New York strategy" \cite{gelman2007} is highly controversial. The aggressive way in whcih the stop-and-frisk practice was being implemented during 2004 to 2012 in NYC was indeed deemed unconstitutional in 2013, violating the fourth and fourteenth amendment {\color{red} Source}


\subsection{Fairness Experiment: Setup}
For our analysis the task is to predict the arrest of a suspect. We compare the following models in terms of fairness and model performance, measured by the difference in true positive rates and the classification accuracy respectively.:
\begin{itemize}
    \item Regular Random Forest
    \item Reweighing to balance disparate impact metric (Pre-Processing)
    \item Classification Fair Logistic Regression With Covariance Constraints Learner (In-Processing)
    \item Equalized Odds Debiasing (Post-Processing)
\end{itemize}
More details about the methods can be found in the \texttt{mlr3} documentation \cite{mlr3_book}.  
For reweighing, see \href{https://mlr3fairness.mlr-org.com/reference/mlr_pipeops_reweighing.html}{mlr3fairness Reweighing}.  
For fair logistic regression, refer to \href{https://rdrr.io/cran/mlr3fairness/man/mlr_learners_classif.fairzlrm.html}{Fair Logistic Regression}.  
For equalized odds, check \href{https://mlr3fairness.mlr-org.com/reference/mlr_pipeops_equalized_odds.html}{Equalized Odds}.  

% Reweighing: https://mlr3fairness.mlr-org.com/reference/mlr_pipeops_reweighing.html
% Fair logistic regression: https://rdrr.io/cran/mlr3fairness/man/mlr_learners_classif.fairzlrm.html
% EOd: https://mlr3fairness.mlr-org.com/reference/mlr_pipeops_equalized_odds.html
% mlr3book: https://mlr3book.mlr-org.com/chapters/chapter14/algorithmic_fairness.html
% https://mlr3fairness.mlr-org.com/#debiasing-methods

\subsection{Data description}
\begin{figure}
  \centering
  \begin{minipage}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../figures/sqf_case_study_plot6.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.49\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../figures/sqf_case_study_plot14.pdf}
  \end{minipage}
  \caption{Bar plot comparing the distribution of ethnic groups across boroughs in the SQF 2023 and NYC from 2020 Census (left). On the right a comparison of the estimated borough-wise crime rate per 100,000 citizens with the ethnic distribution of SQF stops.}
  \label{fig:race_distributions}
\end{figure}

As they were the most recent at the time of writing this paper, we work with the stops from 2023. The raw 2023 dataset consists of 16971 observations and 82 variables. We first discarded all the variables that have more than 20\% missing values, which leaves 34 variables.
From this reduced dataset we filter out the complete cases and end up with 12039 observations. \footnote{Simply discarding the missing values and only training on complete cases is discouraged by \cite{fernando2021}. We opt for this approach regardless, since imputation of the missing values is not straight forward
but treating missing values as an extra category will introduce complications when we implement fairness methods.} \\

We summarize "Black Hispanic" and "Black" into the group "Black" and  "American Indian/ Native American" and "Middle Eastern/ Southwest Asian" into the "Other" category. Black people are by far most often stopped, making up 70\% of the total stops; yet, according to 2020 census data black people make up only 20\% of the city's population \autoref{fig:race_distributions}. At the same time white people form the majority of New York citizens (30\%) but contribute with only 6\% to the stops. 
After 2012 there has been a stark decline in stops and the police is known to focus their attention on high crime areas. Therefore, we further look at each borough. 
The most stops in 2023 occur in Bronx and Brooklyn. Based on report of the NYPD and population statistics from 2020, the Bronx also has the highest estimated crime rate per 100,000 citizens. Manhattan is not far behind in crime rate, but has fewer stops. Note that Bronx and Brooklyn happen to be the boroughs with the highest proportion of black citizens \autoref{fig:race_distributions}.\\

  
Given the historical context of stop-and-frisk, the question arises if a classifier trained on data from the unconstitutional period will perform differently.
We choose data from 2011 as it is the year with the most stops. We carry out the same data cleaning steps for the 2011 data as before, starting with 685724 recorded stops and reducing this to 651567 clean observations. Note that these are more than 50 times more stops than in 2023.
The 2011 data has substantially more low-risk stops, only around 6\% of stops result in an arrest. This is a stark contrast to the 31\% in 2023. In the data, the differences in arrestment rate between groups are slightly lower for 2011 and the highest arrestment rate remains to be for the white group.\\

As features, we select variables that should resemble the information that were available to the officer at the time they made the decision to arrest the person. This includes information about the development of the stop, e.g. whether the person was frisked or a summon issued. We assume that all of these constitute "smaller" hits that happen before an officer chooses the most extreme consequence, an arrest. Additionally, we control for factors, such as the time of the stop or whether the officer was wearing a uniform. This selection of features is inspired by \cite{Badr2022DTFANSP}.

\subsection{Results of the Fairness Experiment}
For the training of the classifiers, we dichotomize the race attribute by grouping "Black" and "Hispanic" as people of colour ("PoC") and "White", "Asian", and "Other" as white ("White"). We run a five-fold cross validation and show the results in \autoref{fig:fairness_experiment}. In the bottom right corner we find fair and accurate classifiers. In terms of fairness reweighing and the equalized odds post-processing method perform best. However, the regular random forest classifier comes close to their fairness performance and performs slightly more accurate. Somewhat surprisingly, it does not make any difference for the fairness if the classifier is trained on 2011 or 2023 data.
We examined the model closer and find that due to the low prevalence in the population, a classifier trained on 2011 data primarily suffers from the highly skewed distribution of arrests. The classifier largely predicts the negative label for \textit{anyone} regardless of race, which overshadows potential fairness concerns. The fairness adjusted logistic regression performs worst in terms of accuracy and fairness.
As the picture could change depending on the chosen fairness metric (y-axis), we also tried out other metrics, such as equalised odds or predictive parity. In all cases the regular random forest does not perform worse in terms of fairness but better in terms of accuracy than most fairness adjusted classifiers.
% result plot fairness experiment
\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{../figures/sqf_case_study_plot3.pdf}
    \caption{Comparison of learners with respect to classification accuracy (x-axis) and equal opportunity (y-axis) across (dots) and aggregated over (crosses) five folds.}
    \label{fig:fairness_experiment}
\end{figure}

Since the classifiers perform similarly, we choose the regular random forest trained on 2023 to examine the model closer.
On the left we plot the prediction score densities for each group in \autoref{fig:fairness_density}. We can see that in general white people tend to have higher predicted probabilities than PoC. The mode for the scores for non-white individuals is around 0.05 while it is around 0.125 for white individuals. The score resembles the probability of being predicted positive (arrested).
On the right \autoref{fig:fairness_density} we plot the absolute difference in selected group fairness metrics.
Exact equality of the group metrics cannot be expected in practice, so it is common to allow for a margin of error $\epsilon$. Taking $\epsilon = 0.05$, the classifier is fair according to each of the selected metrics, though the difference in positive predictive rates is close to 0.05.
For a more nuanced picture, we additionally report the group-wise error metrics in \autoref{tab:groupwise_metrics_2023}.
The true positive rate, false positive rate, and the accuracy is basically identical between the two groups. So the Separation metrics are fulfilled. More ore less notable differences can only be seen in the Sufficiency metrics: the negative predictive values/ positive predictive value.\\

\begin{figure}
    \centering
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/sqf_case_study_plot1.pdf}
    \end{minipage}
    \hfill
    \begin{minipage}{0.49\textwidth}
        \centering
        \includegraphics[width=\textwidth]{../figures/sqf_case_study_plot2.pdf}
    \end{minipage}
    \caption{Fairness prediction density plot (left) showing the density of predictions for the positive class split by "PoC" and "White" individuals. The metrics comparison barplot (right) displays the model's absolute differences across the specified metrics.}
    \label{fig:fairness_density}
\end{figure}

% results table
\begin{table}[ht]
  \centering
  \begin{tabular}{rrrrrrr}
    \hline
   & tpr & npv & fpr & ppv & fdr & acc \\ 
    \hline
    PoC & 0.75 & 0.89 & 0.07 & 0.84 & 0.16 & 0.88 \\ 
    White & 0.74 & 0.85 & 0.06 & 0.89 & 0.11 & 0.86 \\ 
     \hline
  \end{tabular}
  \caption{Groupwise Fairness Metrics (2023)} 
  \label{tab:groupwise_metrics_2023}
\end{table}

All in all, it seems like a classifier trained on SQF data to predict the arrest of a suspect is not discriminatory against PoC. In contrast, it even performs better on many of the common performance metrics for PoC than for white people. \cite{Badr2022DTFANSP} have similar findings.\\
In their study they choose six representative machine learning algorithms (Logistic Regression, Random Forest, Extreme Gradient Boost, Gaussian Naïve Bayes, Support Vector Classifier) to predict the arrest of a suspect. Fairness is measured with six different metrics (Balanced Accuracy, Statistical Parity, Equal Opportunity, Disparate Impact, Avg. Odds Difference, Theil Index) and separate analysis are conducted with sex and race as PA.
They compare the fairness of the regular learner to the fairness of learner with a pre-processing method (reweighing) and a post-processing method (Reject Option-based Classifier). All in all, they find that the regular models to not perform worse in terms of fairness than the fairness adjusted models. This leads them to conclude "[...] that there is no-to-less racial bias that is present in the NYPD Stop-and-Frisk dataset concerning colored and Hispanic individuals."
What both of our case studies have in common is that the models were trained on recent data. We trained our model on 2023 stops and \cite{Badr2022DTFANSP} used 2019 stops. Since the judgement of how stop-and-frisk was implemented in NYC in 2013, the number of stops has decreased significantly and citizens are in generally less often stopped. After 2014 the stops have been consistently kept at a low level. See this website for a visualization and information on the governing police administration at a given period \href{https://www.nyclu.org/data/stop-and-frisk-data}{stop-and-frisk over time}.
\cite{Badr2022DTFANSP} see this as explanation for their results and state "The NYPD has taken crucial steps over the past years and significantly reduced racial and genderbased bias in the stops leading to arrests. This conclusion nullifies the common belief that the NYPD Stop-and-Frisk program is biased toward colored and Hispanic individuals." Is this the whole picture?




