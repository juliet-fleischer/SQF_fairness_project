
% Why did our fairness experiment did not show any substantial disparities?\\
% By predicting the arrest of a person we asked a different fairness questions. Fairness in SQF can bee seen as a two-stage problem. The firs stage: was the person stopped? The second stage: what was the result of the stop? For the whole picture it might be better to go in this order. Our tasks jump directly to the second stage. This is not a mistake per se, but one should be aware that these analyses do not reflect the whole process but only a part of it.\\
% On top of this, the mechanisms behind the selection bias in SQF is twisted in the sense that the historically discriminated group is \textit{more present} in the data. Often the situation is that disadvantaged groups form underrepresented minorities, thus the algorithm oversees them and performs worse on them. In the SQF data, however, the algorithm has plenty of observations from PoC to learn from and less from white people.

Certainly the SQF data comes with interesting questions and challenges. We specifically examined fairness and selection bias, but there are more aspects to explore. Historical bias could also play a role and it would be interesting to see how future studies could incorporate this. Moreover, the impact of the class imbalance in the protected attribute on the fairness of the model could be further investigated. The dataset was rightfully recommended by x, offering research possibilities for various disciplines.\\
After a detailed fairness audit, a fairness experiment with various learner and the review of multiple studies our answer to "Is the stop, question, and frisk practice fair?" remains to be: it is complex.\\
With our work we showed that, before any fairness intervention, it is crucial to formulate a concrete fairness question. It is something entirely different to ask if the stop, question, and frisk practice (as a whole) is fair or whether a classifier to predict the arrest of a person trained on the historical stops is fair.\\
The question we formulate can lead to the design of completely different algorithmic tasks and fairness analysis.


