
\documentclass[12pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\title{Fair Machine Learning}
\author{Translated Lecture}
\date{}

\begin{document}

\maketitle

\section*{Introduction}
**Kapitel 1: Einleitung und Problemstellung**

Start: [00:00:00]  
Ende: [00:02:02]  

Der Vortrag beginnt mit einem Beispiel, bei dem ein Machine-Learning-Algorithmus entwickelt werden soll, um der New Yorker Polizei bei der Reduzierung von Kriminalitätsraten zu helfen. Dabei wird auf potenzielle Gefahren wie Racial Profiling hingewiesen und die Frage aufgeworfen, wie Algorithmen gerecht gestaltet werden können. Das Forschungsfeld Fair Machine Learning wird vorgestellt und die Struktur des Vortrags umrissen.

**Kapitel 2: Grundlagen der Fairness im Machine Learning**

Start: [00:02:02]  
Ende: [00:04:06]  

Es werden zwei grundlegende Konzepte von Fairness erläutert: Gruppenfairness und individuelle Fairness. Gruppenfairness zielt darauf ab, Gleichheit zwischen Gruppen herzustellen, während individuelle Fairness die Gleichheit zwischen Einzelpersonen innerhalb einer Gruppe anstrebt. Zahlreiche Formalisierungen dieser Prinzipien werden diskutiert, wobei der Fokus auf die relevantesten Untergruppen gelegt wird.

**Kapitel 3: Fairnessmetriken im Detail**

Start: [00:04:06]  
Ende: [00:10:24]  

Die verschiedenen Metriken der Gruppenfairness, wie Unabhängigkeit, Separation und Suffizienz, werden detailliert erklärt. Es wird gezeigt, wie durch statistische Parität und Gleichheit der Fehlerraten Fairness zwischen Gruppen erreicht werden kann. Die jeweiligen Vor- und Nachteile sowie die Umsetzbarkeit der Metriken werden ebenfalls behandelt.

**Kapitel 4: Individuelle Fairness und Herausforderungen**

Start: [00:10:24]  
Ende: [00:14:15]  

Hier werden individuelle Fairnesskonzepte wie Fairness through Awareness (FTA) und Fairness through Unawareness (FTU) eingeführt. Dabei wird die Schwierigkeit der Definition von Gleichheit zwischen Individuen diskutiert, ebenso wie Probleme mit Proxy-Variablen, die eine indirekte Diskriminierung ermöglichen können.

**Kapitel 5: Kausale Fairness und Anwendungsmethoden**

Start: [00:14:15]  
Ende: [00:16:07]  

Die dritte große Kategorie der Fairnessdefinitionen, die sich auf kausale Abhängigkeitsstrukturen konzentriert, wird vorgestellt. Diese kausalen Fairnessansätze verstehen und modellieren die zugrunde liegenden Datenstrukturen und bieten dadurch erweiterte Möglichkeiten der Fairness-Messung.

**Kapitel 6: Fairnessmethoden zur Verbesserung von Algorithmen**

Start: [00:16:07]  
Ende: [00:18:05]  

Es werden Methoden zur Verbesserung der Fairness von Algorithmen vorgestellt, die in Preprocessing, Inprocessing und Postprocessing unterteilt werden. Je nach Methode werden Daten angepasst, Optimierungsprobleme modifiziert oder Vorhersagen nachträglich angepasst.

**Kapitel 7: Feedback Loops und Bias im Kontext von Fairness**

Start: [00:18:05]  
Ende: [00:20:34]  

Der Feedbackloop zwischen Menschen, Daten und Algorithmen wird als möglicher Verstärker von Bias dargestellt. Bias kann in jedem Teil des Prozesses – von den Entscheidungen der Menschen über die Datenerhebung bis hin zur Algorithmenentwicklung – eingeführt und verstärkt werden.

**Kapitel 8: Komplexität und Erweiterungen im Fair Machine Learning**

Start: [00:20:34]  
Ende: [00:23:40]  

Die Komplexität des Fair Machine Learning in der Praxis wird angesprochen, insbesondere wenn es mehrere geschützte Attribute und fortgeschrittene Lernaufgaben gibt. Es wird darauf hingewiesen, dass es keine universelle Lösung gibt und dass das Thema Fairness immer im Kontext der spezifischen Daten und Aufgaben betrachtet werden muss. Der Vortrag schließt mit einem Dank und dem Wunsch, das Interesse des Publikums geweckt zu haben.

\end{document}
