% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{kallus}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=2db96cc54afe3fe8c684be53af54a7bf}{%
           family={Kallus},
           familyi={K\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9e1d7c17694696833a7f501d67143a4}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Angela},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{fullhash}{94ee67824833d351384c9ed9a46b9422}
      \strng{bibnamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authorbibnamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authornamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authorfullhash}{94ee67824833d351384c9ed9a46b9422}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairnessadjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-theart fair machine learning can have a ``bias in, bias out'' property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.}
      \field{langid}{english}
      \field{title}{Residual {{Unfairness}} in {{Fair Machine Learning}} from {{Prejudiced Data}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/kallus/Kallus und Zhou - Residual Unfairness in Fair Machine Learning from Prejudiced Data.pdf
      \endverb
    \endentry
    \entry{RambachanBBOEFW}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=85edb53244c29c17ce57830271014b46}{%
           family={Rambachan},
           familyi={R\bibinitperiod},
           given={Ashesh},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=73c1da3266b3c7728f6b1f38a73c7671}{%
           family={Roth},
           familyi={R\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c7db070288680264f6dfa86410e8586f}
      \strng{fullhash}{c7db070288680264f6dfa86410e8586f}
      \strng{bibnamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authorbibnamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authornamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authorfullhash}{c7db070288680264f6dfa86410e8586f}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We evaluate the folk wisdom that algorithmic decision rules trained on data produced by biased human decision-makers necessarily reflect this bias. We consider a setting where training labels are only generated if a biased decision-maker takes a particular action, and so ``biased'' training data arise due to discriminatory selection into the training data. In our baseline model, the more biased the decision-maker is against a group, the more the algorithmic decision rule favors that group. We refer to this phenomenon as bias reversal. We then clarify the conditions that give rise to bias reversal. Whether a prediction algorithm reverses or inherits bias depends critically on how the decision-maker affects the training data as well as the label used in training. We illustrate our main theoretical results in a simulation study applied to the New York City Stop, Question and Frisk dataset.}
      \field{langid}{english}
      \field{title}{Bias {{In}}, {{Bias Out}}? {{Evaluating}} the {{Folk Wisdom}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/RambachanBBOEFW/Rambachan und Roth - Bias In, Bias Out Evaluating the Folk Wisdom.pdf
      \endverb
    \endentry
    \entry{verma2018}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=83905e77def6a8cd568f21d89ce43fc8}{%
           family={Verma},
           familyi={V\bibinitperiod},
           given={Sahil},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e668b98b45e06d4af2230904d870f6e8}{%
           family={Rubin},
           familyi={R\bibinitperiod},
           given={Julia},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Gothenburg Sweden}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{fullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{bibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorbibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authornamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorfullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.}
      \field{booktitle}{Proceedings of the {{International Workshop}} on {{Software Fairness}}}
      \field{isbn}{978-1-4503-5746-3}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{Fairness Definitions Explained}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 7}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1145/3194770.3194776
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Verma2018Fde/Verma und Rubin - 2018 - Fairness definitions explained.pdf
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

