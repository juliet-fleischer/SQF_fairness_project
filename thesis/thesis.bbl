% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{castelnovo2022}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=6c3ab60bce254137083c6d7874df46ed}{%
           family={Castelnovo},
           familyi={C\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a2c7ac5ae0109270013f7cbf8cc88405}{%
           family={Crupi},
           familyi={C\bibinitperiod},
           given={Riccardo},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4db8aa9f51517be9fd795e6356b71550}{%
           family={Greco},
           familyi={G\bibinitperiod},
           given={Greta},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=61e621245c669c73fbaa0363f74bc2df}{%
           family={Regoli},
           familyi={R\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2ddef8f8c0b16e1c67d2d2a34de1f878}{%
           family={Penco},
           familyi={P\bibinitperiod},
           given={Ilaria\bibnamedelima Giuseppina},
           giveni={I\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b8033a196d6f40ea8138545d3d46fcc1}{%
           family={Cosentini},
           familyi={C\bibinitperiod},
           given={Andrea\bibnamedelima Claudio},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{fullhash}{da8eb7772746823d2c9b597e7cd92623}
      \strng{bibnamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authorbibnamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authornamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authorfullhash}{da8eb7772746823d2c9b597e7cd92623}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract In recent years, the problem of addressing fairness in machine learning (ML) and automatic decision making has attracted a lot of attention in the scientific communities dealing with artificial intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a ``fair decision'' in situations impacting individuals in the population. The precise differences, implications and ``orthogonality'' between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.}
      \field{issn}{2045-2322}
      \field{journaltitle}{Scientific Reports}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{title}{A Clarification of the Nuances in the Fairness Metrics Landscape}
      \field{urlday}{23}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{12}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{4209}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41598-022-07939-1
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Castelnovo2022cnfmlb/Castelnovo et al. - 2022 - A clarification of the nuances in the fairness metrics landscape.pdf
      \endverb
    \endentry
    \entry{caton2024}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=1e2b4a1204b7e8f7c7d379a22d93c699}{%
           family={Caton},
           familyi={C\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=df5dadd2d3fbb65e0e1641bab398afcd}{%
           family={Haas},
           familyi={H\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5533b0453780985b76fdb63a216864f2}
      \strng{fullhash}{5533b0453780985b76fdb63a216864f2}
      \strng{bibnamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authorbibnamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authornamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authorfullhash}{5533b0453780985b76fdb63a216864f2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.}
      \field{issn}{0360-0300, 1557-7341}
      \field{journaltitle}{ACM Computing Surveys}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{7}
      \field{shorttitle}{Fairness in {{Machine Learning}}}
      \field{title}{Fairness in {{Machine Learning}}: {{A Survey}}}
      \field{urlday}{23}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{56}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 38}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1145/3616865
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Caton2024FMLSb/Caton und Haas - 2024 - Fairness in Machine Learning A Survey.pdf
      \endverb
    \endentry
    \entry{fernando2021}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=a76fb73b9ba9f02e8cf50125940b9600}{%
           family={Fernando},
           familyi={F\bibinitperiod},
           given={Martínez-Plumed},
           giveni={M\bibinithyphendelim P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0517a7c7351570d28e2489aea09522a0}{%
           family={Cèsar},
           familyi={C\bibinitperiod},
           given={Ferri},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=305a43227c9783317c09fbb0971d41ac}{%
           family={David},
           familyi={D\bibinitperiod},
           given={Nieves},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e870ee962f97ec3db26189c582bfed2d}{%
           family={José},
           familyi={J\bibinitperiod},
           given={Hernández-Orallo},
           giveni={H\bibinithyphendelim O\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{fullhash}{8314c0fc68ea6f434737c6e15b22912c}
      \strng{bibnamehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{authorbibnamehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{authornamehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{authorfullhash}{8314c0fc68ea6f434737c6e15b22912c}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Nowadays, there is an increasing concern in machine learning about the causes underlying unfair decision making, that is, algorithmic decisions discriminating some groups over others, especially with groups that are defined over protected attributes, such as gender, race and nationality. Missing values are one frequent manifestation of all these latent causes: protected groups are more reluctant to give information that could be used against them, sensitive information for some groups can be erased by human operators, or data acquisition may simply be less complete and systematic for minority groups. However, most recent techniques, libraries and experimental results dealing with fairness in machine learning have simply ignored missing data. In this paper, we present the first comprehensive analysis of the relation between missing values and algorithmic fairness for machine learning: (1) we analyse the sources of missing data and bias, mapping the common causes, (2) we find that rows containing missing values are usually fairer than the rest, which should discourage the consideration of missing values as the uncomfortable ugly data that different techniques and libraries for handling algorithmic bias get rid of at the first occasion, (3) we study the trade-off between performance and fairness when the rows with missing values are used (either because the technique deals with them directly or by imputation methods), and (4) we show that the sensitivity of six different machine-learning techniques to missing values is usually low, which reinforces the view that the rows with missing data contribute more to fairness through the other, nonmissing, attributes. We end the paper with a series of recommended procedures about what to do with missing data when aiming for fair decision making.}
      \field{issn}{1098-111X}
      \field{journaltitle}{International Journal of Intelligent Systems}
      \field{langid}{english}
      \field{number}{7}
      \field{shorttitle}{Missing the Missing Values}
      \field{title}{Missing the Missing Values: {{The}} Ugly Duckling of Fairness in Machine Learning}
      \field{urlday}{10}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{36}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{3217\bibrangedash 3258}
      \range{pages}{42}
      \verb{doi}
      \verb 10.1002/int.22415
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Fernando2021Mmvudfml/Fernando et al. - 2021 - Missing the missing values The ugly duckling of fairness in machine learning.pdf;/Users/julietfleischer/Zotero/storage/WDUPKMWG/int.html
      \endverb
      \keyw{algorithmic bias,confirmation bias,data imputation,fairness,missing values,sample bias,survey bias}
    \endentry
    \entry{hardt2016}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b0b481f252064438215c11421a5d102f}{%
           family={Price},
           familyi={P\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b0b481f252064438215c11421a5d102f}{%
           family={Price},
           familyi={P\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=34451c97b5a5bad6fcb6513573d1f94d}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nati},
           giveni={N\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{fullhash}{c715d0ea2bb8e9a21473cec75024f752}
      \strng{bibnamehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{authorbibnamehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{authornamehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{authorfullhash}{c715d0ea2bb8e9a21473cec75024f752}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Equality of {{Opportunity}} in {{Supervised Learning}}}
      \field{urlday}{27}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{29}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/hardt2016/Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf
      \endverb
    \endentry
    \entry{kallus}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=2db96cc54afe3fe8c684be53af54a7bf}{%
           family={Kallus},
           familyi={K\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9e1d7c17694696833a7f501d67143a4}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Angela},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{fullhash}{94ee67824833d351384c9ed9a46b9422}
      \strng{bibnamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authorbibnamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authornamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authorfullhash}{94ee67824833d351384c9ed9a46b9422}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairnessadjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-theart fair machine learning can have a ``bias in, bias out'' property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.}
      \field{langid}{english}
      \field{title}{Residual {{Unfairness}} in {{Fair Machine Learning}} from {{Prejudiced Data}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/kallus/Kallus und Zhou - Residual Unfairness in Fair Machine Learning from Prejudiced Data.pdf
      \endverb
    \endentry
    \entry{kusner}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=37d0d7a3ba0cb7a7253c7b690ce4e8c6}{%
           family={Kusner},
           familyi={K\bibinitperiod},
           given={Matt\bibnamedelima J},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b072a1ed2f750bfeecb0466a94cc478f}{%
           family={Loftus},
           familyi={L\bibinitperiod},
           given={Joshua},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c40407f768dd3e8b296a11750ad77f41}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4047ee6235adca502c24f37317e79d8e}{%
           family={Silva},
           familyi={S\bibinitperiod},
           given={Ricardo},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{fullhash}{31480a6997ada758f14a82daae7e9c2d}
      \strng{bibnamehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{authorbibnamehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{authornamehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{authorfullhash}{31480a6997ada758f14a82daae7e9c2d}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.}
      \field{langid}{english}
      \field{title}{Counterfactual {{Fairness}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/KusnerCF/Kusner et al. - Counterfactual Fairness.pdf
      \endverb
    \endentry
    \entry{RambachanBBOEFW}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=85edb53244c29c17ce57830271014b46}{%
           family={Rambachan},
           familyi={R\bibinitperiod},
           given={Ashesh},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=73c1da3266b3c7728f6b1f38a73c7671}{%
           family={Roth},
           familyi={R\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c7db070288680264f6dfa86410e8586f}
      \strng{fullhash}{c7db070288680264f6dfa86410e8586f}
      \strng{bibnamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authorbibnamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authornamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authorfullhash}{c7db070288680264f6dfa86410e8586f}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We evaluate the folk wisdom that algorithmic decision rules trained on data produced by biased human decision-makers necessarily reflect this bias. We consider a setting where training labels are only generated if a biased decision-maker takes a particular action, and so ``biased'' training data arise due to discriminatory selection into the training data. In our baseline model, the more biased the decision-maker is against a group, the more the algorithmic decision rule favors that group. We refer to this phenomenon as bias reversal. We then clarify the conditions that give rise to bias reversal. Whether a prediction algorithm reverses or inherits bias depends critically on how the decision-maker affects the training data as well as the label used in training. We illustrate our main theoretical results in a simulation study applied to the New York City Stop, Question and Frisk dataset.}
      \field{langid}{english}
      \field{title}{Bias {{In}}, {{Bias Out}}? {{Evaluating}} the {{Folk Wisdom}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/RambachanBBOEFW/Rambachan und Roth - Bias In, Bias Out Evaluating the Folk Wisdom.pdf
      \endverb
    \endentry
    \entry{verma2018}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=83905e77def6a8cd568f21d89ce43fc8}{%
           family={Verma},
           familyi={V\bibinitperiod},
           given={Sahil},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e668b98b45e06d4af2230904d870f6e8}{%
           family={Rubin},
           familyi={R\bibinitperiod},
           given={Julia},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Gothenburg Sweden}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{fullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{bibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorbibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authornamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorfullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.}
      \field{booktitle}{Proceedings of the {{International Workshop}} on {{Software Fairness}}}
      \field{isbn}{978-1-4503-5746-3}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{Fairness Definitions Explained}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 7}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1145/3194770.3194776
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Verma2018Fde/Verma und Rubin - 2018 - Fairness definitions explained.pdf
      \endverb
    \endentry
    \entry{Zafar2017PPNFC}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=651ed744d2d757bbdb4ef8519fdb9745}{%
           family={Zafar},
           familyi={Z\bibinitperiod},
           given={Muhammad\bibnamedelima Bilal},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=86077c699c3bcb4f9fecce395f1e6f41}{%
           family={Valera},
           familyi={V\bibinitperiod},
           given={Isabel},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a189b5a8e9f39b81e8445433ec383ff6}{%
           family={Rodriguez},
           familyi={R\bibinitperiod},
           given={Manuel},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0d02e39696f2420a11d3ee706088466b}{%
           family={Gummadi},
           familyi={G\bibinitperiod},
           given={Krishna},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=89be134d6b955c508c4267860047bdd9}{%
           family={Weller},
           familyi={W\bibinitperiod},
           given={Adrian},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{fullhash}{8a150b1f890c166a574abb59887dff31}
      \strng{bibnamehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{authorbibnamehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{authornamehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{authorfullhash}{8a150b1f890c166a574abb59887dff31}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{From {{Parity}} to {{Preference-based Notions}} of {{Fairness}} in {{Classification}}}
      \field{urlday}{29}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{30}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Zafar2017PPNFC/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

