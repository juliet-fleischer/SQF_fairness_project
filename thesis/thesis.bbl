% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nyt/global//global/global}
    \entry{Badr2022DTFANSP}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=a21359ea89e586fcea9df371eb20ccb7}{%
           family={Badr},
           familyi={B\bibinitperiod},
           given={Youakim},
           giveni={Y\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=f5c184ac68798ea801914f18e49d22bd}{%
           family={Sharma},
           familyi={S\bibinitperiod},
           given={Rahul},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{16d3a351e115c8091ccb4cb7f52de512}
      \strng{fullhash}{16d3a351e115c8091ccb4cb7f52de512}
      \strng{bibnamehash}{16d3a351e115c8091ccb4cb7f52de512}
      \strng{authorbibnamehash}{16d3a351e115c8091ccb4cb7f52de512}
      \strng{authornamehash}{16d3a351e115c8091ccb4cb7f52de512}
      \strng{authorfullhash}{16d3a351e115c8091ccb4cb7f52de512}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Given the increased concern of racial disparities in the stop-and-frisk programs, the New York Police Department ( NYPD ) requires publicly displaying detailed data for all the stops conducted by police authorities, including the suspected offense and race of the suspects. By adopting a public data transparency policy, it becomes possible to investigate racial biases in stop-and-frisk data and demonstrate the benefit of data transparency to approve or disapprove social beliefs and police practices. Thus, data transparency becomes a crucial need in the era of Artificial Intelligence ( AI ), where police and justice increasingly use different AI techniques not only to understand police practices but also to predict recidivism, crimes, and terrorism. In this study, we develop a predictive analytics method, including bias metrics and bias mitigation techniques to analyze the NYPD Stop-and-Frisk datasets and discover whether underline bias patterns are responsible for stops and arrests. In addition, we perform a fairness analysis on two protected attributes, namely, the race and the gender, and investigate their impacts on arrest decisions. We also apply bias mitigation techniques. The experimental results show that the NYPD Stop-and-Frisk dataset is not biased toward colored and Hispanic individuals and thus law enforcement authorities can apply the bias predictive analytics method to inculcate more fair decisions before making any arrests.}
      \field{issn}{1936-1955, 1936-1963}
      \field{journaltitle}{Journal of Data and Information Quality}
      \field{langid}{english}
      \field{month}{6}
      \field{number}{2}
      \field{title}{Data {{Transparency}} and {{Fairness Analysis}} of the {{NYPD Stop-and-Frisk Program}}}
      \field{urlday}{24}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{14}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 14}
      \range{pages}{14}
      \verb{doi}
      \verb 10.1145/3460533
      \endverb
    \endentry
    \entry{barocas}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=cb55e045fbd37698b0b60a60b567ab83}{%
           family={Barocas},
           familyi={B\bibinitperiod},
           given={Solon},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=95dc795a2be14c50fb6a924acc135ff8}{%
           family={Narayanan},
           familyi={N\bibinitperiod},
           given={Arvind},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{f6daa5fbad9e518e58da0fbc508936ba}
      \strng{fullhash}{f6daa5fbad9e518e58da0fbc508936ba}
      \strng{bibnamehash}{f6daa5fbad9e518e58da0fbc508936ba}
      \strng{authorbibnamehash}{f6daa5fbad9e518e58da0fbc508936ba}
      \strng{authornamehash}{f6daa5fbad9e518e58da0fbc508936ba}
      \strng{authorfullhash}{f6daa5fbad9e518e58da0fbc508936ba}
      \field{sortinit}{B}
      \field{sortinithash}{d7095fff47cda75ca2589920aae98399}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{langid}{english}
      \field{title}{Fairness and {{Machine Learning}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/BarocasFML/Barocas et al. - Fairness and Machine Learning.pdf
      \endverb
    \endentry
    \entry{castelnovo2022}{article}{}
      \name{author}{6}{}{%
        {{un=0,uniquepart=base,hash=6c3ab60bce254137083c6d7874df46ed}{%
           family={Castelnovo},
           familyi={C\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a2c7ac5ae0109270013f7cbf8cc88405}{%
           family={Crupi},
           familyi={C\bibinitperiod},
           given={Riccardo},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=4db8aa9f51517be9fd795e6356b71550}{%
           family={Greco},
           familyi={G\bibinitperiod},
           given={Greta},
           giveni={G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=61e621245c669c73fbaa0363f74bc2df}{%
           family={Regoli},
           familyi={R\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=2ddef8f8c0b16e1c67d2d2a34de1f878}{%
           family={Penco},
           familyi={P\bibinitperiod},
           given={Ilaria\bibnamedelima Giuseppina},
           giveni={I\bibinitperiod\bibinitdelim G\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b8033a196d6f40ea8138545d3d46fcc1}{%
           family={Cosentini},
           familyi={C\bibinitperiod},
           given={Andrea\bibnamedelima Claudio},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{fullhash}{da8eb7772746823d2c9b597e7cd92623}
      \strng{bibnamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authorbibnamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authornamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authorfullhash}{da8eb7772746823d2c9b597e7cd92623}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract In recent years, the problem of addressing fairness in machine learning (ML) and automatic decision making has attracted a lot of attention in the scientific communities dealing with artificial intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a ``fair decision'' in situations impacting individuals in the population. The precise differences, implications and ``orthogonality'' between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.}
      \field{issn}{2045-2322}
      \field{journaltitle}{Scientific Reports}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{title}{A Clarification of the Nuances in the Fairness Metrics Landscape}
      \field{urlday}{23}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{12}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{4209}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41598-022-07939-1
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Castelnovo2022cnfmlb/Castelnovo et al. - 2022 - A clarification of the nuances in the fairness metrics landscape.pdf
      \endverb
    \endentry
    \entry{caton2024}{article}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=1e2b4a1204b7e8f7c7d379a22d93c699}{%
           family={Caton},
           familyi={C\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=df5dadd2d3fbb65e0e1641bab398afcd}{%
           family={Haas},
           familyi={H\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5533b0453780985b76fdb63a216864f2}
      \strng{fullhash}{5533b0453780985b76fdb63a216864f2}
      \strng{bibnamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authorbibnamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authornamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authorfullhash}{5533b0453780985b76fdb63a216864f2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.}
      \field{issn}{0360-0300, 1557-7341}
      \field{journaltitle}{ACM Computing Surveys}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{7}
      \field{shorttitle}{Fairness in {{Machine Learning}}}
      \field{title}{Fairness in {{Machine Learning}}: {{A Survey}}}
      \field{urlday}{23}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{56}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 38}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1145/3616865
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Caton2024FMLSb/Caton und Haas - 2024 - Fairness in Machine Learning A Survey.pdf
      \endverb
    \endentry
    \entry{Chouldechova2016FairPW}{article}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=38a289c38744c96713fe118011e7781d}{%
           family={Chouldechova},
           familyi={C\bibinitperiod},
           given={Alexandra},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{38a289c38744c96713fe118011e7781d}
      \strng{fullhash}{38a289c38744c96713fe118011e7781d}
      \strng{bibnamehash}{38a289c38744c96713fe118011e7781d}
      \strng{authorbibnamehash}{38a289c38744c96713fe118011e7781d}
      \strng{authornamehash}{38a289c38744c96713fe118011e7781d}
      \strng{authorfullhash}{38a289c38744c96713fe118011e7781d}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{journaltitle}{Big data}
      \field{title}{Fair prediction with disparate impact: A study of bias in recidivism prediction instruments}
      \field{volume}{5 2}
      \field{year}{2016}
      \field{pages}{153\bibrangedash 163}
      \range{pages}{11}
      \verb{urlraw}
      \verb https://api.semanticscholar.org/CorpusID:1443041
      \endverb
      \verb{url}
      \verb https://api.semanticscholar.org/CorpusID:1443041
      \endverb
    \endentry
    \entry{corbett-davies}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=341a17b1faf8ad74034822ff2f891742}{%
           family={{Corbett-Davies}},
           familyi={C\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=dc87d955c84b91ba3d881af1c1a93666}{%
           family={Gaebler},
           familyi={G\bibinitperiod},
           given={Johann\bibnamedelima D},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a4b30e51359854c0d89aa15a6279a424}{%
           family={Nilforoshan},
           familyi={N\bibinitperiod},
           given={Hamed},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85bd24ef7b2a76727f6692bb1bd9d048}{%
           family={Shroff},
           familyi={S\bibinitperiod},
           given={Ravi},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=769c0f637707fcc5e9ebb71ce29eb5d1}{%
           family={Goel},
           familyi={G\bibinitperiod},
           given={Sharad},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{fullhash}{4544570fa35bc719cb1465901595a433}
      \strng{bibnamehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{authorbibnamehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{authornamehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{authorfullhash}{4544570fa35bc719cb1465901595a433}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labeldatesource}{nodate}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.}
      \field{langid}{english}
      \field{title}{The {{Measure}} and {{Mismeasure}} of {{Fairness}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Corbett-DaviesMMFa/Corbett-Davies et al. - The Measure and Mismeasure of Fairness.pdf
      \endverb
    \endentry
    \entry{favier2023}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=1fe3345cc439e966a11872df3e2ee06c}{%
           family={Favier},
           familyi={F\bibinitperiod},
           given={Marco},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6aab2f9608a64a01a7cdd6501ba9fd5c}{%
           family={Calders},
           familyi={C\bibinitperiod},
           given={Toon},
           giveni={T\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=9f53b89329314a0f5ccf85bb237912be}{%
           family={Pinxteren},
           familyi={P\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=c140dd19e8c320ff999cde7362857b78}{%
           family={Meyer},
           familyi={M\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{7ed25bb12fe1ec3b1c97731a986464b4}
      \strng{fullhash}{63226d30e0e75c47469ef7272ba250aa}
      \strng{bibnamehash}{7ed25bb12fe1ec3b1c97731a986464b4}
      \strng{authorbibnamehash}{7ed25bb12fe1ec3b1c97731a986464b4}
      \strng{authornamehash}{7ed25bb12fe1ec3b1c97731a986464b4}
      \strng{authorfullhash}{63226d30e0e75c47469ef7272ba250aa}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{It is widely accepted that biased data leads to biased and thus potentially unfair models. Therefore, several measures for bias in data and model predictions have been proposed, as well as bias mitigation techniques whose aim is to learn models that are fair by design. Despite the myriad of mitigation techniques developed in the past decade, however, it is still poorly understood under what circumstances which methods work. Recently, Wick et al. showed, with experiments on synthetic data, that there exist situations in which bias mitigation techniques lead to more accurate models when measured on unbiased data. Nevertheless, in the absence of a thorough mathematical analysis, it remains unclear which techniques are effective under what circumstances. We propose to address this problem by establishing relationships between the type of bias and the effectiveness of a mitigation technique, where we categorize the mitigation techniques by the bias measure they optimize. In this paper we illustrate this principle for label and selection bias on the one hand, and demographic parity and ``We're All Equal'' on the other hand. Our theoretical analysis allows to explain the results of Wick et al. and we also show that there are situations where minimizing fairness measures does not result in the fairest possible distribution.}
      \field{issn}{0885-6125, 1573-0565}
      \field{journaltitle}{Machine Learning}
      \field{langid}{english}
      \field{month}{12}
      \field{number}{12}
      \field{shorttitle}{How to Be Fair?}
      \field{title}{How to Be Fair? {{A}} Study of Label and Selection Bias}
      \field{urlday}{5}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{volume}{112}
      \field{year}{2023}
      \field{urldateera}{ce}
      \field{pages}{5081\bibrangedash 5104}
      \range{pages}{24}
      \verb{doi}
      \verb 10.1007/s10994-023-06401-1
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/favier2023/Favier et al. - 2023 - How to be fair A study of label and selection bias.pdf
      \endverb
    \endentry
    \entry{fernando2021}{article}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=a76fb73b9ba9f02e8cf50125940b9600}{%
           family={Fernando},
           familyi={F\bibinitperiod},
           given={Martínez-Plumed},
           giveni={M\bibinithyphendelim P\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0517a7c7351570d28e2489aea09522a0}{%
           family={Cèsar},
           familyi={C\bibinitperiod},
           given={Ferri},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=305a43227c9783317c09fbb0971d41ac}{%
           family={David},
           familyi={D\bibinitperiod},
           given={Nieves},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e870ee962f97ec3db26189c582bfed2d}{%
           family={José},
           familyi={J\bibinitperiod},
           given={Hernández-Orallo},
           giveni={H\bibinithyphendelim O\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{fullhash}{8314c0fc68ea6f434737c6e15b22912c}
      \strng{bibnamehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{authorbibnamehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{authornamehash}{481f96f5fd56976107d5db0082c5ef6c}
      \strng{authorfullhash}{8314c0fc68ea6f434737c6e15b22912c}
      \field{sortinit}{F}
      \field{sortinithash}{2638baaa20439f1b5a8f80c6c08a13b4}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Nowadays, there is an increasing concern in machine learning about the causes underlying unfair decision making, that is, algorithmic decisions discriminating some groups over others, especially with groups that are defined over protected attributes, such as gender, race and nationality. Missing values are one frequent manifestation of all these latent causes: protected groups are more reluctant to give information that could be used against them, sensitive information for some groups can be erased by human operators, or data acquisition may simply be less complete and systematic for minority groups. However, most recent techniques, libraries and experimental results dealing with fairness in machine learning have simply ignored missing data. In this paper, we present the first comprehensive analysis of the relation between missing values and algorithmic fairness for machine learning: (1) we analyse the sources of missing data and bias, mapping the common causes, (2) we find that rows containing missing values are usually fairer than the rest, which should discourage the consideration of missing values as the uncomfortable ugly data that different techniques and libraries for handling algorithmic bias get rid of at the first occasion, (3) we study the trade-off between performance and fairness when the rows with missing values are used (either because the technique deals with them directly or by imputation methods), and (4) we show that the sensitivity of six different machine-learning techniques to missing values is usually low, which reinforces the view that the rows with missing data contribute more to fairness through the other, nonmissing, attributes. We end the paper with a series of recommended procedures about what to do with missing data when aiming for fair decision making.}
      \field{issn}{1098-111X}
      \field{journaltitle}{International Journal of Intelligent Systems}
      \field{langid}{english}
      \field{number}{7}
      \field{shorttitle}{Missing the Missing Values}
      \field{title}{Missing the Missing Values: {{The}} Ugly Duckling of Fairness in Machine Learning}
      \field{urlday}{10}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{36}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{3217\bibrangedash 3258}
      \range{pages}{42}
      \verb{doi}
      \verb 10.1002/int.22415
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Fernando2021Mmvudfml/Fernando et al. - 2021 - Missing the missing values The ugly duckling of fairness in machine learning.pdf;/Users/julietfleischer/Zotero/storage/WDUPKMWG/int.html
      \endverb
      \keyw{algorithmic bias,confirmation bias,data imputation,fairness,missing values,sample bias,survey bias}
    \endentry
    \entry{gelman2007}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=1e1eb60e3b6e222217bf8e9b24070b28}{%
           family={Gelman},
           familyi={G\bibinitperiod},
           given={Andrew},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=a3923e1b9cdc01e1e073dc6b9ffe1b3b}{%
           family={Fagan},
           familyi={F\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d37eeb99a871aab354ceddd1a2f6cf1c}{%
           family={Kiss},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{23a74311177b75079ac994aea3638a07}
      \strng{fullhash}{23a74311177b75079ac994aea3638a07}
      \strng{bibnamehash}{23a74311177b75079ac994aea3638a07}
      \strng{authorbibnamehash}{23a74311177b75079ac994aea3638a07}
      \strng{authornamehash}{23a74311177b75079ac994aea3638a07}
      \strng{authorfullhash}{23a74311177b75079ac994aea3638a07}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{issn}{0162-1459, 1537-274X}
      \field{journaltitle}{Journal of the American Statistical Association}
      \field{langid}{english}
      \field{month}{9}
      \field{number}{479}
      \field{title}{An {{Analysis}} of the {{New York City Police Department}}'s ``{{Stop-and-Frisk}}'' {{Policy}} in the {{Context}} of {{Claims}} of {{Racial Bias}}}
      \field{urlday}{8}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{102}
      \field{year}{2007}
      \field{urldateera}{ce}
      \field{pages}{813\bibrangedash 823}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1198/016214506000001040
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/gelman2007/Gelman et al. - 2007 - An Analysis of the New York City Police Department's “Stop-and-Frisk” Policy in the Context of Claim.pdf
      \endverb
    \endentry
    \entry{lane}{book}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=6cfc641e2ca8f9579534edebf4ffb064}{%
           family={Ghani},
           familyi={G\bibinitperiod},
           given={Rayid},
           giveni={R\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=da8c0cc59b107a3e07208005277b7ea9}{%
           family={Jarmin},
           familyi={J\bibinitperiod},
           given={Ron\bibnamedelima S.},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7c91aa8a97e2bf810febcf40b083a72b}{%
           family={Kreuter},
           familyi={K\bibinitperiod},
           given={Frauke},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=ef1fd4a400dec702b5baeeac0f942be7}{%
           family={Foster},
           familyi={F\bibinitperiod},
           given={Ian},
           giveni={I\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=846f2f7465f0ceb9fa52ff5e6bb0bae9}{%
           family={Lane},
           familyi={L\bibinitperiod},
           given={Julia},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{64d4fcb8216a0215a9b23663926b9fc5}
      \strng{fullhash}{ed33af3b4582e0333768cb1e229cbf0b}
      \strng{bibnamehash}{64d4fcb8216a0215a9b23663926b9fc5}
      \strng{authorbibnamehash}{64d4fcb8216a0215a9b23663926b9fc5}
      \strng{authornamehash}{64d4fcb8216a0215a9b23663926b9fc5}
      \strng{authorfullhash}{ed33af3b4582e0333768cb1e229cbf0b}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{url}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Chapter 11: Bias and Fairness | Big Data and Social Science}
      \field{title}{Chapter 11: Bias and Fairness {|} {Big Data and Social Science}}
      \field{urlday}{15}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/julietfleischer/Zotero/storage/IKNVR93L/chap-bias.html
      \endverb
    \endentry
    \entry{goel2016}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=769c0f637707fcc5e9ebb71ce29eb5d1}{%
           family={Goel},
           familyi={G\bibinitperiod},
           given={Sharad},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=7651b9e8b620ce83ddb4fb2bd7055232}{%
           family={Rao},
           familyi={R\bibinitperiod},
           given={Justin\bibnamedelima M.},
           giveni={J\bibinitperiod\bibinitdelim M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=85bd24ef7b2a76727f6692bb1bd9d048}{%
           family={Shroff},
           familyi={S\bibinitperiod},
           given={Ravi},
           giveni={R\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{416827a33b2091ac55346297d5058a23}
      \strng{fullhash}{416827a33b2091ac55346297d5058a23}
      \strng{bibnamehash}{416827a33b2091ac55346297d5058a23}
      \strng{authorbibnamehash}{416827a33b2091ac55346297d5058a23}
      \strng{authornamehash}{416827a33b2091ac55346297d5058a23}
      \strng{authorfullhash}{416827a33b2091ac55346297d5058a23}
      \field{sortinit}{G}
      \field{sortinithash}{32d67eca0634bf53703493fb1090a2e8}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{issn}{1932-6157}
      \field{journaltitle}{The Annals of Applied Statistics}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{shorttitle}{Precinct or Prejudice?}
      \field{title}{Precinct or Prejudice? {{Understanding}} Racial Disparities in {{New York City}}'s Stop-and-Frisk Policy}
      \field{urlday}{19}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{volume}{10}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{doi}
      \verb 10.1214/15-AOAS897
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Goel2016PpUrdNYCsp/Goel et al. - 2016 - Precinct or prejudice Understanding racial disparities in New York City’s stop-and-frisk policy.pdf
      \endverb
    \endentry
    \entry{hardt2016}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b0b481f252064438215c11421a5d102f}{%
           family={Price},
           familyi={P\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b0b481f252064438215c11421a5d102f}{%
           family={Price},
           familyi={P\bibinitperiod},
           given={Eric},
           giveni={E\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=34451c97b5a5bad6fcb6513573d1f94d}{%
           family={Srebro},
           familyi={S\bibinitperiod},
           given={Nati},
           giveni={N\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{fullhash}{c715d0ea2bb8e9a21473cec75024f752}
      \strng{bibnamehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{authorbibnamehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{authornamehash}{6715d12818335ad532b3aa871d63dcdf}
      \strng{authorfullhash}{c715d0ea2bb8e9a21473cec75024f752}
      \field{sortinit}{H}
      \field{sortinithash}{23a3aa7c24e56cfa16945d55545109b5}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{Equality of {{Opportunity}} in {{Supervised Learning}}}
      \field{urlday}{27}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{29}
      \field{year}{2016}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/hardt2016/Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf
      \endverb
    \endentry
    \entry{kallus2018}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=2db96cc54afe3fe8c684be53af54a7bf}{%
           family={Kallus},
           familyi={K\bibinitperiod},
           given={Nathan},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=b9e1d7c17694696833a7f501d67143a4}{%
           family={Zhou},
           familyi={Z\bibinitperiod},
           given={Angela},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {PMLR}%
      }
      \strng{namehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{fullhash}{94ee67824833d351384c9ed9a46b9422}
      \strng{bibnamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authorbibnamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authornamehash}{94ee67824833d351384c9ed9a46b9422}
      \strng{authorfullhash}{94ee67824833d351384c9ed9a46b9422}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a "bias in, bias out" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.}
      \field{booktitle}{Proceedings of the 35th {{International Conference}} on {{Machine Learning}}}
      \field{issn}{2640-3498}
      \field{langid}{english}
      \field{month}{7}
      \field{title}{Residual {{Unfairness}} in {{Fair Machine Learning}} from {{Prejudiced Data}}}
      \field{urlday}{24}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{2439\bibrangedash 2448}
      \range{pages}{10}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Kallus2018RUFMLPD/Kallus und Zhou - 2018 - Residual Unfairness in Fair Machine Learning from Prejudiced Data 2.pdf;/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Kallus2018RUFMLPD/Kallus und Zhou - 2018 - Residual Unfairness in Fair Machine Learning from Prejudiced Data 3.pdf
      \endverb
    \endentry
    \entry{Khademi2019FADMELC}{inproceedings}{}
      \name{author}{4}{}{%
        {{un=0,uniquepart=base,hash=b55b999ad178707f117666f017a7955a}{%
           family={Khademi},
           familyi={K\bibinitperiod},
           given={Aria},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e1d07770182b45127701c1b12023aa62}{%
           family={Lee},
           familyi={L\bibinitperiod},
           given={Sanghack},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=268f1d53400ccb34afac70143a9e4eeb}{%
           family={Foley},
           familyi={F\bibinitperiod},
           given={David},
           giveni={D\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=81459ef9e663cf418b718d392867309d}{%
           family={Honavar},
           familyi={H\bibinitperiod},
           given={Vasant},
           giveni={V\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {San Francisco CA USA}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{b34e3522dc8b2a57506522c5dff13291}
      \strng{fullhash}{6b343b98a0bdd0de36c4b1a7df848f0a}
      \strng{bibnamehash}{b34e3522dc8b2a57506522c5dff13291}
      \strng{authorbibnamehash}{b34e3522dc8b2a57506522c5dff13291}
      \strng{authornamehash}{b34e3522dc8b2a57506522c5dff13291}
      \strng{authorfullhash}{6b343b98a0bdd0de36c4b1a7df848f0a}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{booktitle}{The {{World Wide Web Conference}}}
      \field{isbn}{978-1-4503-6674-8}
      \field{langid}{english}
      \field{month}{5}
      \field{shorttitle}{Fairness in {{Algorithmic Decision Making}}}
      \field{title}{Fairness in {{Algorithmic Decision Making}}: {{An Excursion Through}} the {{Lens}} of {{Causality}}}
      \field{urlday}{24}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{year}{2019}
      \field{urldateera}{ce}
      \field{pages}{2907\bibrangedash 2914}
      \range{pages}{8}
      \verb{doi}
      \verb 10.1145/3308558.3313559
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Khademi2019FADMELC/Khademi et al. - 2019 - Fairness in Algorithmic Decision Making An Excursion Through the Lens of Causality.pdf
      \endverb
    \endentry
    \entry{kleinberg2017}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=e405ae3c720db1b0e447ff5b601b968f}{%
           family={Kleinberg},
           familyi={K\bibinitperiod},
           given={Jon},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0ad4a50fedec7f3fb2ce43f3f64e62d6}{%
           family={Mullainathan},
           familyi={M\bibinitperiod},
           given={Sendhil},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=94c87c71bf2fc02be222df4cf15f575a}{%
           family={Raghavan},
           familyi={R\bibinitperiod},
           given={Manish},
           giveni={M\bibinitperiod},
           givenun=0}}%
      }
      \list{publisher}{1}{%
        {Schloss Dagstuhl -- Leibniz-Zentrum für Informatik}%
      }
      \strng{namehash}{55804740a6bc42af2c3d658a3da3ce37}
      \strng{fullhash}{55804740a6bc42af2c3d658a3da3ce37}
      \strng{bibnamehash}{55804740a6bc42af2c3d658a3da3ce37}
      \strng{authorbibnamehash}{55804740a6bc42af2c3d658a3da3ce37}
      \strng{authornamehash}{55804740a6bc42af2c3d658a3da3ce37}
      \strng{authorfullhash}{55804740a6bc42af2c3d658a3da3ce37}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Recent discussion in the public sphere about algorithmic classification has involved tension between competing notions of what it means for a probabilistic classification to be fair to different groups. We formalize three fairness conditions that lie at the heart of these debates, and we prove that except in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously. Moreover, even satisfying all three conditions approximately requires that the data lie in an approximate version of one of the constrained special cases identified by our theorem. These results suggest some of the ways in which key notions of fairness are incompatible with each other, and hence provide a framework for thinking about the trade-offs between them.}
      \field{isbn}{9783959770293}
      \field{issn}{1868-8969}
      \field{journaltitle}{LIPIcs, Volume 67, ITCS 2017}
      \field{langid}{english}
      \field{title}{Inherent {{Trade-Offs}} in the {{Fair Determination}} of {{Risk Scores}}}
      \field{urlday}{27}
      \field{urlmonth}{2}
      \field{urlyear}{2025}
      \field{volume}{67}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{43:1\bibrangedash 43:23}
      \range{pages}{-1}
      \verb{doi}
      \verb 10.4230/LIPICS.ITCS.2017.43
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/kleinberg2017/Kleinberg et al. - 2017 - Inherent Trade-Offs in the Fair Determination of Risk Scores.pdf
      \endverb
      \keyw{algorithmic fairness,calibration,risk tools}
    \endentry
    \entry{Lakkaraju2017SLPEAPPU}{inproceedings}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=e19a29d61e30f8ddedc3fb2cbd2f3cdb}{%
           family={Lakkaraju},
           familyi={L\bibinitperiod},
           given={Himabindu},
           giveni={H\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e405ae3c720db1b0e447ff5b601b968f}{%
           family={Kleinberg},
           familyi={K\bibinitperiod},
           given={Jon},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=900d107125ff0ca84698cb909e4f6c51}{%
           family={Leskovec},
           familyi={L\bibinitperiod},
           given={Jure},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=358307c361a8a70eb2780e904d602421}{%
           family={Ludwig},
           familyi={L\bibinitperiod},
           given={Jens},
           giveni={J\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=0ad4a50fedec7f3fb2ce43f3f64e62d6}{%
           family={Mullainathan},
           familyi={M\bibinitperiod},
           given={Sendhil},
           giveni={S\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Halifax NS Canada}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{fc5fa27dfcfed83a1a4e4a7cb16ed71e}
      \strng{fullhash}{6178877235c42695b3853d26347a368a}
      \strng{bibnamehash}{fc5fa27dfcfed83a1a4e4a7cb16ed71e}
      \strng{authorbibnamehash}{fc5fa27dfcfed83a1a4e4a7cb16ed71e}
      \strng{authornamehash}{fc5fa27dfcfed83a1a4e4a7cb16ed71e}
      \strng{authorfullhash}{6178877235c42695b3853d26347a368a}
      \field{sortinit}{L}
      \field{sortinithash}{7c47d417cecb1f4bd38d1825c427a61a}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{Evaluating whether machines improve on human performance is one of the central questions of machine learning. However, there are many domains where the data is selectively labeled in the sense that the observed outcomes are themselves a consequence of the existing choices of the human decision-makers. For instance, in the context of judicial bail decisions, we observe the outcome of whether a defendant fails to return for their court appearance only if the human judge decides to release the defendant on bail. This selective labeling makes it harder to evaluate predictive models as the instances for which outcomes are observed do not represent a random sample of the population. Here we propose a novel framework for evaluating the performance of predictive models on selectively labeled data. We develop an approach called contraction which allows us to compare the performance of predictive models and human decision-makers without resorting to counterfactual inference. Our methodology harnesses the heterogeneity of human decision-makers and facilitates effective evaluation of predictive models even in the presence of unmeasured confounders (unobservables) which influence both human decisions and the resulting outcomes. Experimental results on real world datasets spanning diverse domains such as health care, insurance, and criminal justice demonstrate the utility of our evaluation metric in comparing human decisions and machine predictions.}
      \field{booktitle}{Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}}
      \field{isbn}{978-1-4503-4887-4}
      \field{langid}{english}
      \field{month}{8}
      \field{shorttitle}{The {{Selective Labels Problem}}}
      \field{title}{The {{Selective Labels Problem}}: {{Evaluating Algorithmic Predictions}} in the {{Presence}} of {{Unobservables}}}
      \field{urlday}{25}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{year}{2017}
      \field{urldateera}{ce}
      \field{pages}{275\bibrangedash 284}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/3097983.3098066
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Lakkaraju2017SLPEAPPU/Lakkaraju et al. - 2017 - The Selective Labels Problem Evaluating Algorithmic Predictions in the Presence of Unobservables.pdf
      \endverb
    \endentry
    \entry{makhlouf2021}{article}{}
      \name{author}{3}{}{%
        {{un=0,uniquepart=base,hash=99da3194dcde78db72606c8a724f180f}{%
           family={Makhlouf},
           familyi={M\bibinitperiod},
           given={Karima},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=d388bc0c628674b3a47873f32b0084f1}{%
           family={Zhioua},
           familyi={Z\bibinitperiod},
           given={Sami},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=1be391749a3de49eae8b6aa3838278db}{%
           family={Palamidessi},
           familyi={P\bibinitperiod},
           given={Catuscia},
           giveni={C\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{3882a33a1565267fefd1d99b6052dd1e}
      \strng{fullhash}{3882a33a1565267fefd1d99b6052dd1e}
      \strng{bibnamehash}{3882a33a1565267fefd1d99b6052dd1e}
      \strng{authorbibnamehash}{3882a33a1565267fefd1d99b6052dd1e}
      \strng{authornamehash}{3882a33a1565267fefd1d99b6052dd1e}
      \strng{authorfullhash}{3882a33a1565267fefd1d99b6052dd1e}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine Learning (ML) based predictive systems are increasingly used to support decisions with a critical impact on individuals' lives such as college admission, job hiring, child custody, criminal risk assessment, etc. As a result, fairness emerged as an important requirement to guarantee that ML predictive systems do not discriminate against specific individuals or entire sub-populations, in particular, minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions of fairness have been introduced in the literature. This paper is a survey of fairness notions that, unlike other surveys in the literature, addresses the question of ``which notion of fairness is most suited to a given real-world scenario and why?''. Our attempt to answer this question consists in (1) identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzing the behavior of each fairness notion, and then (3) fitting these two elements to recommend the most suitable fairness notion in every specific setup. The results are summarized in a decision diagram that can be used by practitioners and policy makers to navigate the relatively large catalogue of ML fairness notions.}
      \field{issn}{1931-0145, 1931-0153}
      \field{journaltitle}{ACM SIGKDD Explorations Newsletter}
      \field{langid}{english}
      \field{month}{5}
      \field{number}{1}
      \field{title}{On the {{Applicability}} of {{Machine Learning Fairness Notions}}}
      \field{urlday}{1}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{23}
      \field{year}{2021}
      \field{urldateera}{ce}
      \field{pages}{14\bibrangedash 23}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1145/3468507.3468511
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Makhlouf2021AMLFN/Makhlouf et al. - 2021 - On the Applicability of Machine Learning Fairness Notions.pdf
      \endverb
    \endentry
    \entry{mehrabi2022}{article}{}
      \name{author}{5}{}{%
        {{un=0,uniquepart=base,hash=e49a830517defa430474d9c2b5c22d83}{%
           family={Mehrabi},
           familyi={M\bibinitperiod},
           given={Ninareh},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=6fc2e21e9c786462d54f7e2eeae61c19}{%
           family={Morstatter},
           familyi={M\bibinitperiod},
           given={Fred},
           giveni={F\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=125158a659e4885af8901c5155a4e57a}{%
           family={Saxena},
           familyi={S\bibinitperiod},
           given={Nripsuta},
           giveni={N\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=8aea0a421db201a447f5fec22e73a92f}{%
           family={Lerman},
           familyi={L\bibinitperiod},
           given={Kristina},
           giveni={K\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=73b781905368dfe2cc66cef5e33e0d32}{%
           family={Galstyan},
           familyi={G\bibinitperiod},
           given={Aram},
           giveni={A\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{fullhash}{70df8b815290d4668300a842f0dfe8bc}
      \strng{bibnamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authorbibnamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authornamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authorfullhash}{70df8b815290d4668300a842f0dfe8bc}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.}
      \field{issn}{0360-0300, 1557-7341}
      \field{journaltitle}{ACM Computing Surveys}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{6}
      \field{title}{A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}}
      \field{urlday}{7}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{54}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 35}
      \range{pages}{35}
      \verb{doi}
      \verb 10.1145/3457607
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/mehrabi2022/Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf
      \endverb
    \endentry
    \entry{mlr3_book}{incollection}{}
      \name{author}{1}{}{%
        {{un=0,uniquepart=base,hash=c2fc2334f775f39d53c050871c587dc4}{%
           family={Pfisterer},
           familyi={P\bibinitperiod},
           given={Florian},
           giveni={F\bibinitperiod},
           givenun=0}}%
      }
      \name{editor}{4}{}{%
        {{hash=c5695b464b6059e7047553d017275f65}{%
           family={Bischl},
           familyi={B\bibinitperiod},
           given={Bernd},
           giveni={B\bibinitperiod}}}%
        {{hash=2db4d382d99a2d9177badfb9a18d85d5}{%
           family={Sonabend},
           familyi={S\bibinitperiod},
           given={Raphael},
           giveni={R\bibinitperiod}}}%
        {{hash=d989545591ff0b2bb124972c34d7811b}{%
           family={Kotthoff},
           familyi={K\bibinitperiod},
           given={Lars},
           giveni={L\bibinitperiod}}}%
        {{hash=081c44d48de07decc0eb61e6b652e349}{%
           family={Lang},
           familyi={L\bibinitperiod},
           given={Michel},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {CRC Press}%
      }
      \strng{namehash}{c2fc2334f775f39d53c050871c587dc4}
      \strng{fullhash}{c2fc2334f775f39d53c050871c587dc4}
      \strng{bibnamehash}{c2fc2334f775f39d53c050871c587dc4}
      \strng{authorbibnamehash}{c2fc2334f775f39d53c050871c587dc4}
      \strng{authornamehash}{c2fc2334f775f39d53c050871c587dc4}
      \strng{authorfullhash}{c2fc2334f775f39d53c050871c587dc4}
      \strng{editorbibnamehash}{57c73c265eb28377149f924698b3a3da}
      \strng{editornamehash}{57c73c265eb28377149f924698b3a3da}
      \strng{editorfullhash}{8f4d0e3482183e3314fc1c8797341c1d}
      \field{sortinit}{P}
      \field{sortinithash}{ff3bcf24f47321b42cb156c2cc8a8422}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Applied Machine Learning Using {m}lr3 in {R}}
      \field{title}{Algorithmic Fairness}
      \field{year}{2024}
      \verb{urlraw}
      \verb https://mlr3book.mlr-org.com/algorithmic_fairness.html
      \endverb
      \verb{url}
      \verb https://mlr3book.mlr-org.com/algorithmic_fairness.html
      \endverb
    \endentry
    \entry{rambachan2016}{misc}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=85edb53244c29c17ce57830271014b46}{%
           family={Rambachan},
           familyi={R\bibinitperiod},
           given={Ashesh},
           giveni={A\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=73c1da3266b3c7728f6b1f38a73c7671}{%
           family={Roth},
           familyi={R\bibinitperiod},
           given={Jonathan},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \strng{namehash}{c7db070288680264f6dfa86410e8586f}
      \strng{fullhash}{c7db070288680264f6dfa86410e8586f}
      \strng{bibnamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authorbibnamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authornamehash}{c7db070288680264f6dfa86410e8586f}
      \strng{authorfullhash}{c7db070288680264f6dfa86410e8586f}
      \field{sortinit}{R}
      \field{sortinithash}{5e1c39a9d46ffb6bebd8f801023a9486}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We evaluate the folk wisdom that algorithmic decision rules trained on data produced by biased human decision-makers necessarily reflect this bias. We consider a setting where training labels are only generated if a biased decision-maker takes a particular action, and so ``biased'' training data arise due to discriminatory selection into the training data. In our baseline model, the more biased the decision-maker is against a group, the more the algorithmic decision rule favors that group. We refer to this phenomenon as bias reversal. We then clarify the conditions that give rise to bias reversal. Whether a prediction algorithm reverses or inherits bias depends critically on how the decision-maker affects the training data as well as the label used in training. We illustrate our main theoretical results in a simulation study applied to the New York City Stop, Question and Frisk dataset.}
      \field{langid}{english}
      \field{title}{Bias In, Bias Out? Evaluating the Folk Wisdom}
      \field{year}{2016}
      \verb{urlraw}
      \verb https://drops.dagstuhl.de/storage/00lipics/lipics-vol156-forc2020/LIPIcs.FORC.2020.6/LIPIcs.FORC.2020.6.pdf
      \endverb
      \verb{url}
      \verb https://drops.dagstuhl.de/storage/00lipics/lipics-vol156-forc2020/LIPIcs.FORC.2020.6/LIPIcs.FORC.2020.6.pdf
      \endverb
    \endentry
    \entry{verma2018}{inproceedings}{}
      \name{author}{2}{}{%
        {{un=0,uniquepart=base,hash=83905e77def6a8cd568f21d89ce43fc8}{%
           family={Verma},
           familyi={V\bibinitperiod},
           given={Sahil},
           giveni={S\bibinitperiod},
           givenun=0}}%
        {{un=0,uniquepart=base,hash=e668b98b45e06d4af2230904d870f6e8}{%
           family={Rubin},
           familyi={R\bibinitperiod},
           given={Julia},
           giveni={J\bibinitperiod},
           givenun=0}}%
      }
      \list{location}{1}{%
        {Gothenburg Sweden}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{fullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{bibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorbibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authornamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorfullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{extradatescope}{labelyear}
      \field{labeldatesource}{}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.}
      \field{booktitle}{Proceedings of the {{International Workshop}} on {{Software Fairness}}}
      \field{isbn}{978-1-4503-5746-3}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{Fairness Definitions Explained}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 7}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1145/3194770.3194776
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Verma2018Fde/Verma und Rubin - 2018 - Fairness definitions explained.pdf
      \endverb
    \endentry
  \enddatalist
  \missing{Fabris_2022}
\endrefsection
\endinput

