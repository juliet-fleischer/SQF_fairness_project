@article{Fabris_2022,
	doi = {10.1007/s10618-022-00854-z},
	url = {https://doi.org/10.1007%2Fs10618-022-00854-z},
	year = 2022,
	month = {sep},
	publisher = {Springer Science and Business Media {LLC}},
	volume = {36},
	number = {6},
	pages = {2074--2152},
	author = {Alessandro Fabris and Stefano Messina and Gianmaria Silvello and Gian Antonio Susto},
	title = {Algorithmic fairness datasets: the story so far},
	journal = {Data Mining and Knowledge Discovery}
}
@inproceedings{Binns2020,
author = {Binns, Reuben},
title = {On the apparent conflict between individual and group fairness},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3372864},
doi = {10.1145/3351095.3372864},
abstract = {A distinction has been drawn in fair machine learning research between 'group' and 'individual' fairness measures. Many technical research papers assume that both are important, but conflicting, and propose ways to minimise the trade-offs between these measures. This paper argues that this apparent conflict is based on a misconception. It draws on discussions from within the fair machine learning research, and from political and legal philosophy, to argue that individual and group fairness are not fundamentally in conflict. First, it outlines accounts of egalitarian fairness which encompass plausible motivations for both group and individual fairness, thereby suggesting that there need be no conflict in principle. Second, it considers the concept of individual justice, from legal philosophy and jurisprudence, which seems similar but actually contradicts the notion of individual fairness as proposed in the fair machine learning literature. The conclusion is that the apparent conflict between individual and group fairness is more of an artefact of the blunt application of fairness measures, rather than a matter of conflicting principles. In practice, this conflict may be resolved by a nuanced consideration of the sources of 'unfairness' in a particular deployment context, and the carefully justified application of measures to mitigate it.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {514â€“524},
numpages = {11},
keywords = {discrimination, fairness, individual fairness, justice, machine learning, statistical parity},
location = {Barcelona, Spain},
series = {FAT* '20}
}

@article{Chouldechova2016FairPW,
  title={Fair prediction with disparate impact: A study of bias in recidivism prediction instruments},
  author={Alexandra Chouldechova},
  journal={Big data},
  year={2016},
  volume={5 2},
  pages={
          153-163
        },
  url={https://api.semanticscholar.org/CorpusID:1443041}
}

@incollection{mlr3_book, 
  author = "Florian Pfisterer", 
  title = "Algorithmic Fairness",
  booktitle = "Applied Machine Learning Using {m}lr3 in {R}",
  publisher = "CRC Press", year = "2024",
  editor = "Bernd Bischl and Raphael Sonabend and Lars Kotthoff and Michel Lang", 
  url = "https://mlr3book.mlr-org.com/algorithmic_fairness.html"
}


@book{lane,
  title = {Chapter 11: Bias and Fairness {\textbar} {Big Data and Social Science}},
  author = {Rayid Ghani and Ron S. Jarmin and Frauke Kreuter and Ian Foster and Julia Lane},
  urldate = {2025-01-15},
  abstract = {Chapter 11: Bias and Fairness | Big Data and Social Science},
  file = {/Users/julietfleischer/Zotero/storage/IKNVR93L/chap-bias.html}
}

@misc{rambachan2016,
  author = {Rambachan, Ashesh and Roth, Jonathan},
  title = {Bias In, Bias Out? Evaluating the Folk Wisdom},
  year = {2016},
  abstract = {We evaluate the folk wisdom that algorithmic decision rules trained on data produced by biased human decision-makers necessarily reflect this bias. We consider a setting where training labels are only generated if a biased decision-maker takes a particular action, and so ``biased'' training data arise due to discriminatory selection into the training data. In our baseline model, the more biased the decision-maker is against a group, the more the algorithmic decision rule favors that group. We refer to this phenomenon as bias reversal. We then clarify the conditions that give rise to bias reversal. Whether a prediction algorithm reverses or inherits bias depends critically on how the decision-maker affects the training data as well as the label used in training. We illustrate our main theoretical results in a simulation study applied to the New York City Stop, Question and Frisk dataset.},
  langid = {english},
  url = {https://drops.dagstuhl.de/storage/00lipics/lipics-vol156-forc2020/LIPIcs.FORC.2020.6/LIPIcs.FORC.2020.6.pdf}
}

@article{corbett-davies,
author = {Corbett-Davies, Sam and Gaebler, Johann D. and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
title = {The measure and mismeasure of fairness},
year = {2023},
issue_date = {January 2023},
publisher = {JMLR.org},
volume = {24},
number = {1},
issn = {1532-4435},
abstract = {The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.},
journal = {J. Mach. Learn. Res.},
month = jan,
articleno = {312},
numpages = {117},
keywords = {fair machine learning, consequentialism, discrimination}
}

@book{barocas-hardt-narayanan,
  title = {Fairness and Machine Learning: Limitations and Opportunities},
  author = {Solon Barocas and Moritz Hardt and Arvind Narayanan},
  publisher = {MIT Press},
  year = {2023}
}
