@misc{Ahmadzadeh2022MCSDPEM,
  title = {Measuring {{Class-Imbalance Sensitivity}} of {{Deterministic Performance Evaluation Metrics}}},
  author = {Ahmadzadeh, Azim and Angryk, Rafal A.},
  year = {2022},
  month = jun,
  number = {arXiv:2206.09981},
  eprint = {2206.09981},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.09981},
  urldate = {2024-12-23},
  abstract = {The class-imbalance issue is intrinsic to many real-world machine learning tasks, particularly to the rare-event classification problems. Although the impact and treatment of imbalanced data is widely known, the magnitude of a metric's sensitivity to class imbalance has attracted little attention. As a result, often the sensitive metrics are dismissed while their sensitivity may only be marginal. In this paper, we introduce an intuitive evaluation framework that quantifies metrics' sensitivity to the class imbalance. Moreover, we reveal an interesting fact that there is a logarithmic behavior in metrics' sensitivity meaning that the higher imbalance ratios are associated with the lower sensitivity of metrics. Our framework builds an intuitive understanding of the class-imbalance impact on metrics. We believe this can help avoid many common mistakes, specially the less-emphasized and incorrect assumption that all metrics' quantities are comparable under different class-imbalance ratios.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Machine Learning},
  note = {Comment: The 29th IEEE International Conference on Image Processing, IEEE ICIP 2022},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Ahmadzadeh2022MCSDPEM/Ahmadzadeh und Angryk - 2022 - Measuring Class-Imbalance Sensitivity of Deterministic Performance Evaluation Metrics.pdf}
}

@article{Badr2022DTFANSP,
  title = {Data {{Transparency}} and {{Fairness Analysis}} of the {{NYPD Stop-and-Frisk Program}}},
  author = {Badr, Youakim and Sharma, Rahul},
  year = {2022},
  month = jun,
  journal = {Journal of Data and Information Quality},
  volume = {14},
  number = {2},
  pages = {1--14},
  issn = {1936-1955, 1936-1963},
  doi = {10.1145/3460533},
  urldate = {2024-12-24},
  abstract = {Given the increased concern of racial disparities in the stop-and-frisk programs, the               New York Police Department               (               NYPD               ) requires publicly displaying detailed data for all the stops conducted by police authorities, including the suspected offense and race of the suspects. By adopting a public data transparency policy, it becomes possible to investigate racial biases in stop-and-frisk data and demonstrate the benefit of data transparency to approve or disapprove social beliefs and police practices. Thus, data transparency becomes a crucial need in the era of               Artificial Intelligence               (               AI               ), where police and justice increasingly use different AI techniques not only to understand police practices but also to predict recidivism, crimes, and terrorism. In this study, we develop a predictive analytics method, including bias metrics and bias mitigation techniques to analyze the NYPD Stop-and-Frisk datasets and discover whether underline bias patterns are responsible for stops and arrests. In addition, we perform a fairness analysis on two protected attributes, namely, the race and the gender, and investigate their impacts on arrest decisions. We also apply bias mitigation techniques. The experimental results show that the NYPD Stop-and-Frisk dataset is not biased toward colored and Hispanic individuals and thus law enforcement authorities can apply the bias predictive analytics method to inculcate more fair decisions before making any arrests.},
  langid = {english}
}

@article{barocas,
  title = {Fairness and {{Machine Learning}}},
  author = {Barocas, Solon and Hardt, Moritz and Narayanan, Arvind},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/BarocasFML/Barocas et al. - Fairness and Machine Learning.pdf}
}

@article{Brzezinski2024PFMCVCIPGR,
  title = {Properties of {{Fairness Measures}} in the {{Context}} of {{Varying Class Imbalance}} and {{Protected Group Ratios}}},
  author = {Brzezinski, Dariusz and Stachowiak, Julia and Stefanowski, Jerzy and Szczech, Izabela and Susmaga, Robert and Aksenyuk, Sofya and Ivashka, Uladzimir and Yasinskyi, Oleksandr},
  year = {2024},
  month = jun,
  journal = {ACM Trans. Knowl. Discov. Data},
  volume = {18},
  number = {7},
  pages = {170:1--170:18},
  issn = {1556-4681},
  doi = {10.1145/3654659},
  urldate = {2024-12-23},
  abstract = {Society is increasingly relying on predictive models in fields like criminal justice, credit risk management, and hiring. To prevent such automated systems from discriminating against people belonging to certain groups, fairness measures have become a crucial component in socially relevant applications of machine learning. However, existing fairness measures have been designed to assess the bias between predictions for protected groups without considering the imbalance in the classes of the target variable. Current research on the potential effect of class imbalance on fairness focuses on practical applications rather than dataset-independent measure properties. In this article, we study the general properties of fairness measures for changing class and protected group proportions. For this purpose, we analyze the probability mass functions of six of the most popular group fairness measures. We also measure how the probability of achieving perfect fairness changes for varying class imbalance ratios. Moreover, we relate the dataset-independent properties of fairness measures described in this work to classifier fairness in real-life tasks. Our results show that measures such as Equal Opportunity and Positive Predictive Parity are more sensitive to changes in class imbalance than Accuracy Equality. These findings can help guide researchers and practitioners in choosing the most appropriate fairness measures for their classification problems.}
}

@article{castelnovo2022,
  title = {A Clarification of the Nuances in the Fairness Metrics Landscape},
  author = {Castelnovo, Alessandro and Crupi, Riccardo and Greco, Greta and Regoli, Daniele and Penco, Ilaria Giuseppina and Cosentini, Andrea Claudio},
  year = {2022},
  month = mar,
  journal = {Scientific Reports},
  volume = {12},
  number = {1},
  pages = {4209},
  issn = {2045-2322},
  doi = {10.1038/s41598-022-07939-1},
  urldate = {2024-12-23},
  abstract = {Abstract             In recent years, the problem of addressing fairness in machine learning (ML) and automatic decision making has attracted a lot of attention in the scientific communities dealing with artificial intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a ``fair decision'' in situations impacting individuals in the population. The precise differences, implications and ``orthogonality'' between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Castelnovo2022cnfmlb/Castelnovo et al. - 2022 - A clarification of the nuances in the fairness metrics landscape.pdf}
}

@article{caton,
  title = {Impact of {{Imputation Strategies}} on {{Fairness}} in {{Machine Learning}}},
  author = {Caton, Simon and Caton, Simon and Ie, Ucd and Malisetty, Saiteja and Edu, Unomaha and Haas, Christian and Haas, Christian and At, Wu Ac},
  abstract = {Research on Fairness and Bias Mitigation in Machine Learning often uses a set of reference datasets for the design and evaluation of novel approaches or definitions. While these datasets are well structured and useful for the comparison of various approaches, they do not reflect that datasets commonly used in real-world applications can have missing values. When such missing values are encountered, the use of imputation strategies is commonplace. However, as imputation strategies potentially alter the distribution of data they can also affect the performance, and potentially the fairness, of the resulting predictions, a topic not yet well understood in the fairness literature. In this article, we investigate the impact of different imputation strategies on classical performance and fairness in classification settings. We find that the selected imputation strategy, along with other factors including the type of classification algorithm, can significantly affect performance and fairness outcomes. The results of our experiments indicate that the choice of imputation strategy is an important factor when considering fairness in Machine Learning. We also provide some insights and guidance for researchers to help navigate imputation approaches for fairness.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Caton et al. - Impact of Imputation Strategies on Fairness in Machine Learning.pdf}
}

@article{caton2024,
  title = {Fairness in {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Fairness in {{Machine Learning}}},
  author = {Caton, Simon and Haas, Christian},
  year = {2024},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {7},
  pages = {1--38},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3616865},
  urldate = {2024-12-23},
  abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Caton2024FMLSb/Caton und Haas - 2024 - Fairness in Machine Learning A Survey.pdf}
}

@article{corbett-davies,
  title = {The {{Measure}} and {{Mismeasure}} of {{Fairness}}},
  author = {{Corbett-Davies}, Sam and Gaebler, Johann D and Nilforoshan, Hamed and Shroff, Ravi and Goel, Sharad},
  abstract = {The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Corbett-DaviesMMFa/Corbett-Davies et al. - The Measure and Mismeasure of Fairness.pdf}
}

@article{deng2023,
  title = {{{FIFA}}: {{MAKING FAIRNESS MORE GENERALIZABLE IN CLASSIFIERS TRAINED ON IMBALANCED DATA}}},
  author = {Deng, Zhun and Zhang, Jiayao and Zhang, Linjun and Ye, Ting and Coley, Yates and Su, Weijie J and Zou, James},
  year = {2023},
  abstract = {Algorithmic fairness plays an important role in machine learning and imposing fairness constraints during learning is a common approach. However, many datasets are imbalanced in certain label classes (e.g. "healthy") and sensitive subgroups (e.g. "older patients"). Empirically, this imbalance leads to a lack of generalizability not only of classification, but also of fairness properties, especially in over-parameterized models. For example, fairness-aware training may ensure equalized odds (EO) on the training data, but EO is far from being satisfied on new users. In this paper, we propose a theoretically-principled, yet Flexible approach that is Imbalance-Fairness-Aware (FIFA). Specifically, FIFA encourages both classification and fairness generalization and can be flexibly combined with many existing fair learning methods with logits-based losses. While our main focus is on EO, FIFA can be directly applied to achieve equalized opportunity (EqOpt); and under certain conditions, it can also be applied to other fairness notions. We demonstrate the power of FIFA by combining it with a popular fair classification algorithm, and the resulting algorithm achieves significantly better fairness generalization on several real-world datasets.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Deng2023FMFMGCTID/Deng et al. - 2023 - FIFA MAKING FAIRNESS MORE GENERALIZABLE IN CLASSIFIERS TRAINED ON IMBALANCED DATA.pdf}
}

@inproceedings{dwork2012,
  title = {Fairness through Awareness},
  booktitle = {Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}},
  author = {Dwork, Cynthia and Hardt, Moritz and Pitassi, Toniann and Reingold, Omer and Zemel, Richard},
  year = {2012},
  month = jan,
  pages = {214--226},
  publisher = {ACM},
  address = {Cambridge Massachusetts},
  doi = {10.1145/2090236.2090255},
  urldate = {2024-12-29},
  abstract = {We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of ``fair affirmative action,'' which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.},
  isbn = {978-1-4503-1115-1},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Dwork2012Faa/Dwork et al. - 2012 - Fairness through awareness.pdf}
}

@article{fagan2022,
  title = {No {{Runs}}, {{Few Hits}}, and {{Many Errors}}: {{Street Stops}}, {{Bias}}, and {{Proactive Policing}}},
  shorttitle = {No {{Runs}}, {{Few Hits}}, and {{Many Errors}}},
  author = {Fagan, Jeffrey},
  year = {2022},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4052926},
  urldate = {2024-11-11},
  abstract = {Equilibrium models of racial discrimination in law enforcement encounters suggest that in the absence of racial discrimination, the proportion of searches yielding evidence of illegal activity (the hit rate) will be equal across races. Searches that disproportionately target one racial group, resulting in a relatively low hit rate, are inefficient and suggest bias. An unbiased officer who is seeking to maximize her hit rate would reduce the number of unproductive stops toward a group with the lower hit rate. An unbiased policing regime would generate no differences in hit rates between groups.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Fagan2022NRFHMESSBPP/Fagan - 2022 - No Runs, Few Hits, and Many Errors Street Stops, Bias, and Proactive Policing.pdf}
}

@article{farrell2022,
  title = {Use of Force during Stop and Frisks: {{Examining}} the Role of Suspect Demeanor and Race},
  shorttitle = {Use of Force during Stop and Frisks},
  author = {Farrell, Chelsea},
  year = {2022},
  month = sep,
  journal = {Journal of Criminal Justice},
  volume = {82},
  pages = {102001},
  issn = {00472352},
  doi = {10.1016/j.jcrimjus.2022.102001},
  urldate = {2024-11-11},
  abstract = {Purpose: This study examines predictors of suspect demeanor, the relationship between demeanor and use of force during stop and frisks, and whether this relationship varies across suspect race. Methods: Multilevel models using 2018--2020 NYPD Stop, Question, and Frisk data are used to examine demeanor and assess the role of race and demeanor on use of force. Results: Results demonstrate that race is significantly associated with officer perceptions of suspect demeanor and demeanor impacts use of weapon and non-weapon force. Specifically, calm suspects are less likely to experience both forms of force and upset, angry, and non-compliant/aggressive suspects are more likely to experience both forms of force. Although race was found to significantly predict demeanor, suspect race does not appear to moderate the impact of demeanor on use of force with a few exceptions. For example, non-compliant/aggressive Black and Hispanic suspects are more likely to face non-weapon force compared to non-compliant/aggressive White suspects and calm Black suspects are more likely to face weapon force compared to calm White suspects. Conclusion: Suspect demeanor is missing from stop and frisk research, but findings suggest that race may shape perceptions of demeanor and demeanor may play an important role in understanding use of force.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Farrell2022UfsfErsdr/Farrell - 2022 - Use of force during stop and frisks Examining the role of suspect demeanor and race.pdf}
}

@article{fernando2021,
  title = {Missing the Missing Values: {{The}} Ugly Duckling of Fairness in Machine Learning},
  shorttitle = {Missing the Missing Values},
  author = {Fernando, Mart{\'i}nez-Plumed and C{\`e}sar, Ferri and David, Nieves and Jos{\'e}, Hern{\'a}ndez-Orallo},
  year = {2021},
  journal = {International Journal of Intelligent Systems},
  volume = {36},
  number = {7},
  pages = {3217--3258},
  issn = {1098-111X},
  doi = {10.1002/int.22415},
  urldate = {2024-12-10},
  abstract = {Nowadays, there is an increasing concern in machine learning about the causes underlying unfair decision making, that is, algorithmic decisions discriminating some groups over others, especially with groups that are defined over protected attributes, such as gender, race and nationality. Missing values are one frequent manifestation of all these latent causes: protected groups are more reluctant to give information that could be used against them, sensitive information for some groups can be erased by human operators, or data acquisition may simply be less complete and systematic for minority groups. However, most recent techniques, libraries and experimental results dealing with fairness in machine learning have simply ignored missing data. In this paper, we present the first comprehensive analysis of the relation between missing values and algorithmic fairness for machine learning: (1) we analyse the sources of missing data and bias, mapping the common causes, (2) we find that rows containing missing values are usually fairer than the rest, which should discourage the consideration of missing values as the uncomfortable ugly data that different techniques and libraries for handling algorithmic bias get rid of at the first occasion, (3) we study the trade-off between performance and fairness when the rows with missing values are used (either because the technique deals with them directly or by imputation methods), and (4) we show that the sensitivity of six different machine-learning techniques to missing values is usually low, which reinforces the view that the rows with missing data contribute more to fairness through the other, nonmissing, attributes. We end the paper with a series of recommended procedures about what to do with missing data when aiming for fair decision making.},
  langid = {english},
  keywords = {algorithmic bias,confirmation bias,data imputation,fairness,missing values,sample bias,survey bias},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Fernando2021Mmvudfml/Fernando et al. - 2021 - Missing the missing values The ugly duckling of fairness in machine learning.pdf;/Users/julietfleischer/Zotero/storage/WDUPKMWG/int.html}
}

@article{gelman2007,
  title = {An {{Analysis}} of the {{New York City Police Department}}'s ``{{Stop-and-Frisk}}'' {{Policy}} in the {{Context}} of {{Claims}} of {{Racial Bias}}},
  author = {Gelman, Andrew and Fagan, Jeffrey and Kiss, Alex},
  year = {2007},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {102},
  number = {479},
  pages = {813--823},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000001040},
  urldate = {2025-01-08},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/gelman2007/Gelman et al. - 2007 - An Analysis of the New York City Police Department's “Stop-and-Frisk” Policy in the Context of Claim.pdf}
}

@article{glaser2024,
  title = {Disrupting the {{Effects}} of {{Implicit Bias}}: {{The Case}} of {{Discretion}} \& {{Policing}}},
  shorttitle = {Disrupting the {{Effects}} of {{Implicit Bias}}},
  author = {Glaser, Jack},
  year = {2024},
  month = mar,
  journal = {Daedalus},
  volume = {153},
  number = {1},
  pages = {151--173},
  issn = {0011-5266, 1548-6192},
  doi = {10.1162/daed_a_02053},
  urldate = {2024-11-11},
  abstract = {Abstract             Police departments tend to address operational challenges with training approaches, and implicit bias in policing is no exception. However, psychological scientists have found that implicit biases are very difficult to reduce in any lasting, meaningful way. Because they are difficult to change, and nearly impossible for the decision-maker to recognize, training to raise awareness or teach corrective strategies is unlikely to succeed. Recent empirical assessments of implicit bias trainings have shown, at best, no effect on racial disparities in officers' actions in the field. In the absence of effective training, a promising near-term approach for reducing racial disparities in policing is to reduce the frequency of actions most vulnerable to the influence of bias. Specifically, actions that allow relatively high discretion are most likely to be subject to bias-driven errors. Several cases across different policing domains reveal that when discretion is constrained in stop-and-search decisions, the impact of racial bias on searches markedly declines.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Glaser2024DEIBCDP/Glaser - 2024 - Disrupting the Effects of Implicit Bias The Case of Discretion & Policing.pdf}
}

@article{goel2016,
  title = {Precinct or Prejudice? {{Understanding}} Racial Disparities in {{New York City}}'s Stop-and-Frisk Policy},
  shorttitle = {Precinct or Prejudice?},
  author = {Goel, Sharad and Rao, Justin M. and Shroff, Ravi},
  year = {2016},
  month = mar,
  journal = {The Annals of Applied Statistics},
  volume = {10},
  number = {1},
  issn = {1932-6157},
  doi = {10.1214/15-AOAS897},
  urldate = {2024-11-19},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Goel2016PpUrdNYCsp/Goel et al. - 2016 - Precinct or prejudice Understanding racial disparities in New York City’s stop-and-frisk policy.pdf}
}

@article{grgic-hlaca2018,
  title = {Beyond {{Distributive Fairness}} in {{Algorithmic Decision Making}}: {{Feature Selection}} for {{Procedurally Fair Learning}}},
  shorttitle = {Beyond {{Distributive Fairness}} in {{Algorithmic Decision Making}}},
  author = {{Grgi{\'c}-Hla{\v c}a}, Nina and Zafar, Muhammad Bilal and Gummadi, Krishna P. and Weller, Adrian},
  year = {2018},
  month = apr,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11296},
  urldate = {2024-11-19},
  abstract = {With widespread use of machine learning methods in numerous domains involving humans, several studies have raised questions about the potential for unfairness towards certain individuals or groups. A number of recent works have proposed methods to measure and eliminate unfairness from machine learning models. However, most of this work has focused on only one dimension of fair decision making: distributive fairness, i.e., the fairness of the decision outcomes. In this work, we leverage the rich literature on organizational justice and focus on another dimension of fair decision making: procedural fairness, i.e., the fairness of the decision making process. We propose measures for procedural fairness that consider the input features used in the decision process, and evaluate the moral judgments of humans regarding the use of these features. We operationalize these measures on two real world datasets using human surveys on the Amazon Mechanical Turk (AMT) platform, demonstrating that our measures capture important properties of procedurally fair decision making. We provide fast submodular mechanisms to optimize the tradeoff between procedural fairness and prediction accuracy. On our datasets, we observe empirically that procedural fairness may be achieved with little cost to outcome fairness, but that some loss of accuracy is unavoidable.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Grgic-Hlaca2018DFADMFSPFL/Grgić-Hlača et al. - 2018 - Beyond Distributive Fairness in Algorithmic Decision Making Feature Selection for Procedurally Fair.pdf}
}

@inproceedings{hardt2016,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Hardt, Moritz and Price, Eric and Price, Eric and Srebro, Nati},
  year = {2016},
  volume = {29},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-01-27},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/hardt2016/Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf}
}

@misc{hardt2016a,
  title = {Equality of {{Opportunity}} in {{Supervised Learning}}},
  author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
  year = {2016},
  month = oct,
  number = {arXiv:1610.02413},
  eprint = {1610.02413},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1610.02413},
  urldate = {2025-01-27},
  abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests. We illustrate our notion using a case study of FICO credit scores.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/hardt2016a/Hardt et al. - 2016 - Equality of Opportunity in Supervised Learning.pdf;/Users/julietfleischer/Zotero/storage/T68QGMWK/1610.html}
}

@article{Hort2024BMMLCCS,
  title = {Bias {{Mitigation}} for {{Machine Learning Classifiers}}: {{A Comprehensive Survey}}},
  shorttitle = {Bias {{Mitigation}} for {{Machine Learning Classifiers}}},
  author = {Hort, Max and Chen, Zhenpeng and Zhang, Jie M. and Harman, Mark and Sarro, Federica},
  year = {2024},
  month = jun,
  journal = {ACM Journal on Responsible Computing},
  volume = {1},
  number = {2},
  pages = {1--52},
  issn = {2832-0565},
  doi = {10.1145/3631326},
  urldate = {2024-12-28},
  abstract = {This article provides a comprehensive survey of bias mitigation methods for achieving fairness in Machine Learning (ML) models. We collect a total of 341 publications concerning bias mitigation for ML classifiers. These methods can be distinguished based on their intervention procedure (i.e., pre-processing, in-processing, post-processing) and the technique they apply. We investigate how existing bias mitigation methods are evaluated in the literature. In particular, we consider datasets, metrics, and benchmarking. Based on the gathered insights (e.g., What is the most popular fairness metric? How many datasets are used for evaluating bias mitigation methods?), we hope to support practitioners in making informed choices when developing and evaluating new bias mitigation methods.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Hort2024BMMLCCS/Hort et al. - 2024 - Bias Mitigation for Machine Learning Classifiers A Comprehensive Survey.pdf}
}

@inproceedings{Islam2022DMLEAEFC,
  title = {Through the {{Data Management Lens}}: {{Experimental Analysis}} and {{Evaluation}} of {{Fair Classification}}},
  shorttitle = {Through the {{Data Management Lens}}},
  booktitle = {Proceedings of the 2022 {{International Conference}} on {{Management}} of {{Data}}},
  author = {Islam, Maliha Tashfia and Fariha, Anna and Meliou, Alexandra and Salimi, Babak},
  year = {2022},
  month = jun,
  pages = {232--246},
  publisher = {ACM},
  address = {Philadelphia PA USA},
  doi = {10.1145/3514221.3517841},
  urldate = {2024-12-28},
  isbn = {978-1-4503-9249-5},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Islam2022DMLEAEFC/Islam et al. - 2022 - Through the Data Management Lens Experimental Analysis and Evaluation of Fair Classification.pdf}
}

@article{kallus,
  title = {Residual {{Unfairness}} in {{Fair Machine Learning}} from {{Prejudiced Data}}},
  author = {Kallus, Nathan and Zhou, Angela},
  abstract = {Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairnessadjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-theart fair machine learning can have a ``bias in, bias out'' property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/kallus/Kallus und Zhou - Residual Unfairness in Fair Machine Learning from Prejudiced Data.pdf}
}

@inproceedings{kallus2018,
  title = {Residual {{Unfairness}} in {{Fair Machine Learning}} from {{Prejudiced Data}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Kallus, Nathan and Zhou, Angela},
  year = {2018},
  month = jul,
  pages = {2439--2448},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-12-24},
  abstract = {Recent work in fairness in machine learning has proposed adjusting for fairness by equalizing accuracy metrics across groups and has also studied how datasets affected by historical prejudices may lead to unfair decision policies. We connect these lines of work and study the residual unfairness that arises when a fairness-adjusted predictor is not actually fair on the target population due to systematic censoring of training data by existing biased policies. This scenario is particularly common in the same applications where fairness is a concern. We characterize theoretically the impact of such censoring on standard fairness metrics for binary classifiers and provide criteria for when residual unfairness may or may not appear. We prove that, under certain conditions, fairness-adjusted classifiers will in fact induce residual unfairness that perpetuates the same injustices, against the same groups, that biased the data to begin with, thus showing that even state-of-the-art fair machine learning can have a "bias in, bias out" property. When certain benchmark data is available, we show how sample reweighting can estimate and adjust fairness metrics while accounting for censoring. We use this to study the case of Stop, Question, and Frisk (SQF) and demonstrate that attempting to adjust for fairness perpetuates the same injustices that the policy is infamous for.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Kallus2018RUFMLPD/Kallus und Zhou - 2018 - Residual Unfairness in Fair Machine Learning from Prejudiced Data 2.pdf;/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Kallus2018RUFMLPD/Kallus und Zhou - 2018 - Residual Unfairness in Fair Machine Learning from Prejudiced Data 3.pdf}
}

@inproceedings{Khademi2019FADMELC,
  title = {Fairness in {{Algorithmic Decision Making}}: {{An Excursion Through}} the {{Lens}} of {{Causality}}},
  shorttitle = {Fairness in {{Algorithmic Decision Making}}},
  booktitle = {The {{World Wide Web Conference}}},
  author = {Khademi, Aria and Lee, Sanghack and Foley, David and Honavar, Vasant},
  year = {2019},
  month = may,
  pages = {2907--2914},
  publisher = {ACM},
  address = {San Francisco CA USA},
  doi = {10.1145/3308558.3313559},
  urldate = {2024-12-24},
  isbn = {978-1-4503-6674-8},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Khademi2019FADMELC/Khademi et al. - 2019 - Fairness in Algorithmic Decision Making An Excursion Through the Lens of Causality.pdf}
}

@article{kusner,
  title = {Counterfactual {{Fairness}}},
  author = {Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
  abstract = {Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/KusnerCF/Kusner et al. - Counterfactual Fairness.pdf}
}

@inproceedings{Lakkaraju2017SLPEAPPU,
  title = {The {{Selective Labels Problem}}: {{Evaluating Algorithmic Predictions}} in the {{Presence}} of {{Unobservables}}},
  shorttitle = {The {{Selective Labels Problem}}},
  booktitle = {Proceedings of the 23rd {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Lakkaraju, Himabindu and Kleinberg, Jon and Leskovec, Jure and Ludwig, Jens and Mullainathan, Sendhil},
  year = {2017},
  month = aug,
  pages = {275--284},
  publisher = {ACM},
  address = {Halifax NS Canada},
  doi = {10.1145/3097983.3098066},
  urldate = {2024-12-25},
  abstract = {Evaluating whether machines improve on human performance is one of the central questions of machine learning. However, there are many domains where the data is selectively labeled in the sense that the observed outcomes are themselves a consequence of the existing choices of the human decision-makers. For instance, in the context of judicial bail decisions, we observe the outcome of whether a defendant fails to return for their court appearance only if the human judge decides to release the defendant on bail. This selective labeling makes it harder to evaluate predictive models as the instances for which outcomes are observed do not represent a random sample of the population. Here we propose a novel framework for evaluating the performance of predictive models on selectively labeled data. We develop an approach called contraction which allows us to compare the performance of predictive models and human decision-makers without resorting to counterfactual inference. Our methodology harnesses the heterogeneity of human decision-makers and facilitates effective evaluation of predictive models even in the presence of unmeasured confounders (unobservables) which influence both human decisions and the resulting outcomes. Experimental results on real world datasets spanning diverse domains such as health care, insurance, and criminal justice demonstrate the utility of our evaluation metric in comparing human decisions and machine predictions.},
  isbn = {978-1-4503-4887-4},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Lakkaraju2017SLPEAPPU/Lakkaraju et al. - 2017 - The Selective Labels Problem Evaluating Algorithmic Predictions in the Presence of Unobservables.pdf}
}

@book{lane,
  title = {Chapter 11 {{Bias}} and {{Fairness}} {\textbar} {{Big Data}} and {{Social Science}}},
  author = {Lane, Rayid Ghani, Ron S. Jarmin, Frauke Kreuter {and} Julia, Ian Foster},
  urldate = {2025-01-15},
  abstract = {Chapter 11 Bias and Fairness {\textbar} Big Data and Social Science},
  file = {/Users/julietfleischer/Zotero/storage/IKNVR93L/chap-bias.html}
}

@article{makhlouf2021,
  title = {On the {{Applicability}} of {{Machine Learning Fairness Notions}}},
  author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
  year = {2021},
  month = may,
  journal = {ACM SIGKDD Explorations Newsletter},
  volume = {23},
  number = {1},
  pages = {14--23},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/3468507.3468511},
  urldate = {2024-12-01},
  abstract = {Machine Learning (ML) based predictive systems are increasingly used to support decisions with a critical impact on individuals' lives such as college admission, job hiring, child custody, criminal risk assessment, etc. As a result, fairness emerged as an important requirement to guarantee that ML predictive systems do not discriminate against specific individuals or entire sub-populations, in particular, minorities. Given the inherent subjectivity of viewing the concept of fairness, several notions of fairness have been introduced in the literature. This paper is a survey of fairness notions that, unlike other surveys in the literature, addresses the question of ``which notion of fairness is most suited to a given real-world scenario and why?''. Our attempt to answer this question consists in (1) identifying the set of fairness-related characteristics of the real-world scenario at hand, (2) analyzing the behavior of each fairness notion, and then (3) fitting these two elements to recommend the most suitable fairness notion in every specific setup. The results are summarized in a decision diagram that can be used by practitioners and policy makers to navigate the relatively large catalogue of ML fairness notions.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Makhlouf2021AMLFN/Makhlouf et al. - 2021 - On the Applicability of Machine Learning Fairness Notions.pdf}
}

@misc{MarcoScutari[autcre]2020fFMML,
  title = {Fairml: {{Fair Models}} in {{Machine Learning}}},
  shorttitle = {Fairml},
  author = {{Marco Scutari [aut, cre]}},
  year = {2020},
  month = aug,
  pages = {0.8},
  publisher = {Comprehensive R Archive Network},
  doi = {10.32614/CRAN.package.fairml},
  urldate = {2024-12-09},
  abstract = {Fair machine learning regression models which take sensitive attributes into account in model estimation. Currently implementing Komiyama et al. (2018)  {$<$}http://proceedings.mlr.press/v80/komiyama18a/komiyama18a.pdf{$>$}, Zafar et al. (2019) {$<$}https://www.jmlr.org/papers/volume20/18-262/18-262.pdf{$>$} and my own approach from Scutari, Panero and Proissl (2022) {$<$}https://link.springer.com/content/pdf/10.1007/s11222-022-10143-w.pdf{$>$} that uses ridge regression to enforce fairness.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/MarcoScutari[autcre]2020fFMML/Marco Scutari [aut, cre] - 2020 - fairml Fair Models in Machine Learning.pdf}
}

@article{mehrabi2022,
  title = {A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}},
  author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
  year = {2022},
  month = jul,
  journal = {ACM Computing Surveys},
  volume = {54},
  number = {6},
  pages = {1--35},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3457607},
  urldate = {2025-01-07},
  abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/mehrabi2022/Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf}
}

@article{milner2016,
  title = {Black and {{Hispanic Men Perceived}} to {{Be Large Are}} at {{Increased Risk}} for {{Police Frisk}}, {{Search}}, and {{Force}}},
  author = {Milner, Adrienne N. and George, Brandon J. and Allison, David B.},
  editor = {Maher, Brion},
  year = {2016},
  month = jan,
  journal = {PLOS ONE},
  volume = {11},
  number = {1},
  pages = {e0147158},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0147158},
  urldate = {2024-11-11},
  abstract = {Social justice issues remain some of the most pressing problems in the United States. One aspect of social justice involves the differential treatment of demographic groups in the criminal justice system. While data consistently show that Blacks and Hispanics are often treated differently than Whites, one understudied aspect of these disparities is how police officers' assessments of suspects' size affects their decisions. Using over 3 million cases from the New York Police Department (NYPD) Stop, Question, and Frisk (SQF) Database, 2006--2013, this study is the first to explore suspects' race, perceived size, and police treatment. Results indicate that tall and heavy black and Hispanic men are at the greatest risk for frisk or search. Tall and heavy suspects are at increased risk for experiencing police force, with black and Hispanic men being more likely to experience force than white men across size categories.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Milner2016BHMPBLAIRPFSF/Milner et al. - 2016 - Black and Hispanic Men Perceived to Be Large Are at Increased Risk for Police Frisk, Search, and For.pdf}
}

@article{Pessach2023RFML,
  title = {A {{Review}} on {{Fairness}} in {{Machine Learning}}},
  author = {Pessach, Dana and Shmueli, Erez},
  year = {2023},
  month = mar,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {3},
  pages = {1--44},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3494672},
  urldate = {2024-12-28},
  abstract = {An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence and machine learning (ML) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans, and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop ML algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision making may be inherently prone to unfairness, even when there is no intention for it. This article presents an overview of the main concepts of identifying, measuring, and improving algorithmic fairness when using ML algorithms, focusing primarily on classification tasks. The article begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process, and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, toward a better understanding of which mechanisms should be used in different scenarios. The article ends by reviewing several emerging research sub-fields of algorithmic fairness, beyond classification.},
  langid = {english}
}

@article{RambachanBBOEFW,
  title = {Bias {{In}}, {{Bias Out}}? {{Evaluating}} the {{Folk Wisdom}}},
  author = {Rambachan, Ashesh and Roth, Jonathan},
  abstract = {We evaluate the folk wisdom that algorithmic decision rules trained on data produced by biased human decision-makers necessarily reflect this bias. We consider a setting where training labels are only generated if a biased decision-maker takes a particular action, and so ``biased'' training data arise due to discriminatory selection into the training data. In our baseline model, the more biased the decision-maker is against a group, the more the algorithmic decision rule favors that group. We refer to this phenomenon as bias reversal. We then clarify the conditions that give rise to bias reversal. Whether a prediction algorithm reverses or inherits bias depends critically on how the decision-maker affects the training data as well as the label used in training. We illustrate our main theoretical results in a simulation study applied to the New York City Stop, Question and Frisk dataset.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/RambachanBBOEFW/Rambachan und Roth - Bias In, Bias Out Evaluating the Folk Wisdom.pdf}
}

@article{ravishankar2023,
  title = {Provable {{Detection}} of {{Propagating Sampling Bias}} in {{Prediction Models}}},
  author = {Ravishankar, Pavan and Mo, Qingyu and McFowland Iii, Edward and Neill, Daniel B.},
  year = {2023},
  month = jun,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {37},
  number = {8},
  pages = {9562--9569},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v37i8.26144},
  urldate = {2025-01-08},
  abstract = {With an increased focus on incorporating fairness in machine learning models, it becomes imperative not only to assess and mitigate bias at each stage of the machine learning pipeline but also to understand the downstream impacts of bias across stages. Here we consider a general, but realistic, scenario in which a predictive model is learned from (potentially biased) training data, and model predictions are assessed post-hoc for fairness by some auditing method. We provide a theoretical analysis of how a specific form of data bias, differential sampling bias, propagates from the data stage to the prediction stage. Unlike prior work, we evaluate the downstream impacts of data biases quantitatively rather than qualitatively and prove theoretical guarantees for detection. Under reasonable assumptions, we quantify how the amount of bias in the model predictions varies as a function of the amount of differential sampling bias in the data, and at what point this bias becomes provably detectable by the auditor. Through experiments on two criminal justice datasets-- the well-known COMPAS dataset and historical data from NYPD's stop and frisk policy-- we demonstrate that the theoretical results hold in practice even when our assumptions are relaxed.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/ravishankar2023/Ravishankar et al. - 2023 - Provable Detection of Propagating Sampling Bias in Prediction Models.pdf}
}

@book{ridgeway2007,
  title = {Analysis of Racial Disparities in the {{New York Police Department}}'s Stop, Question, and Frisk Practices},
  author = {Ridgeway, Greg},
  year = {2007},
  series = {Technical Report},
  number = {TR-534-NYCPF},
  publisher = {RAND Corporation},
  address = {Santa Monica, CA},
  collaborator = {{Rand Corporation}},
  isbn = {978-0-8330-4515-7},
  langid = {english},
  lccn = {HV8148.N52 R54 2007},
  keywords = {Discrimination in law enforcement,New York (N.Y.),New York (State) New York,Police,Police Department,Stop and frisk (Law enforcement)},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/ridgeway2007/Ridgeway - 2007 - Analysis of racial disparities in the New York Police Department's stop, question, and frisk practic.pdf}
}

@article{sewell,
  title = {Crime and {{Enforcement Activity}} in {{New York City}}},
  author = {Sewell, Keechant},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/sewell/Sewell - Crime and Enforcement Activity in New York City.pdf}
}

@article{tang2023,
  title = {What-Is and {{How-to}} for {{Fairness}} in {{Machine Learning}}: {{A Survey}}, {{Reflection}}, and {{Perspective}}},
  shorttitle = {What-Is and {{How-to}} for {{Fairness}} in {{Machine Learning}}},
  author = {Tang, Zeyu and Zhang, Jiji and Zhang, Kun},
  year = {2023},
  month = dec,
  journal = {ACM Computing Surveys},
  volume = {55},
  number = {13s},
  pages = {1--37},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3597199},
  urldate = {2024-12-23},
  abstract = {We review and reflect on fairness notions proposed in machine learning literature and make an attempt to draw connections to arguments in moral and political philosophy, especially theories of justice. We survey dynamic fairness inquiries and further consider the long-term impact induced by current prediction and decision. We present a flowchart that encompasses implicit assumptions and expected outcomes of different fairness inquiries on the data-generating process, the predicted outcome, and the induced impact, respectively. We demonstrate the importance of matching the mission (what kind of fairness to enforce) and the means (which appropriate fairness spectrum to analyze) to fulfill the intended purpose.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Tang2023WHFMLSRP/Tang et al. - 2023 - What-is and How-to for Fairness in Machine Learning A Survey, Reflection, and Perspective.pdf}
}

@misc{vaswani2023,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2023},
  month = aug,
  number = {arXiv:1706.03762},
  eprint = {1706.03762},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1706.03762},
  urldate = {2024-12-24},
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  note = {Comment: 15 pages, 5 figures},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Vaswani2023AAYN/Vaswani et al. - 2023 - Attention Is All You Need.pdf;/Users/julietfleischer/Zotero/storage/IYR4TQN8/1706.html}
}

@inproceedings{verma2018,
  title = {Fairness Definitions Explained},
  booktitle = {Proceedings of the {{International Workshop}} on {{Software Fairness}}},
  author = {Verma, Sahil and Rubin, Julia},
  year = {2018},
  month = may,
  pages = {1--7},
  publisher = {ACM},
  address = {Gothenburg Sweden},
  doi = {10.1145/3194770.3194776},
  urldate = {2024-11-16},
  abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
  isbn = {978-1-4503-5746-3},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Verma2018Fde/Verma und Rubin - 2018 - Fairness definitions explained.pdf}
}

@article{weisburd2016,
  title = {Do {{Stop}}, {{Question}}, and {{Frisk Practices Deter Crime}}?},
  author = {Weisburd, David and Wooditch, Alese and Weisburd, Sarit and Yang, Sue-Ming},
  year = {2016},
  journal = {Criminology \& Public Policy},
  volume = {15},
  number = {1},
  pages = {31--56},
  issn = {1745-9133},
  doi = {10.1111/1745-9133.12172},
  urldate = {2024-12-01},
  abstract = {Research Summary Existing studies examining the crime impacts of stop, question, and frisks (SQFs) have focused on large geographic areas. Weisburd, Telep, and Lawton (2014) suggested that SQFs in New York City (NYC) were highly concentrated at crime hot spots, implying that a microlevel unit of analysis may be more appropriate. The current study aims to address the limitations of prior studies by exploring the impact of SQFs on daily and weekly crime incidents in NYC at a microgeographic level. The findings suggest that SQFs produce a significant yet modest deterrent effect on crime. Policy Implications These findings support those who argue that SQFs deter crime. Nonetheless, it is not clear whether other policing strategies may have similar or even stronger crime-control outcomes. In turn, the level of SQFs needed to produce meaningful crime reductions are costly in terms of police time and are potentially harmful to police legitimacy.},
  langid = {english},
  file = {/Users/julietfleischer/Zotero/storage/A4RRRHXT/1745-9133.html}
}

@article{zafar,
  title = {Fairness {{Constraints}}: {{A Flexible Approach}} for {{Fair Classification}}},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and {Gomez-Rodriguez}, Manuel and Gummadi, Krishna P},
  abstract = {Algorithmic decision making is employed in an increasing number of real-world applications to aid human decision making. While it has shown considerable promise in terms of improved decision accuracy, in some scenarios, its outcomes have been also shown to impose an unfair (dis)advantage on people from certain social groups (e.g., women, blacks). In this context, there is a need for computational techniques to limit unfairness in algorithmic decision making. In this work, we take a step forward to fulfill that need and introduce a flexible constraint-based framework to enable the design of fair margin-based classifiers. The main technical innovation of our framework is a general and intuitive measure of decision boundary unfairness, which serves as a tractable proxy to several of the most popular computational definitions of unfairness from the literature. Leveraging our measure, we can reduce the design of fair margin-based classifiers to adding tractable constraints on their decision boundaries. Experiments on multiple synthetic and real-world datasets show that our framework is able to successfully limit unfairness, often at a small cost in terms of accuracy.},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/ZafarFCFAFC/Zafar et al. - Fairness Constraints A Flexible Approach for Fair Classiﬁcation.pdf}
}

@inproceedings{Zafar2017FDTDILCDM,
  title = {Fairness {{Beyond Disparate Treatment}} \& {{Disparate Impact}}: {{Learning Classification}} without {{Disparate Mistreatment}}},
  shorttitle = {Fairness {{Beyond Disparate Treatment}} \& {{Disparate Impact}}},
  booktitle = {Proceedings of the 26th {{International Conference}} on {{World Wide Web}}},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and Gomez Rodriguez, Manuel and Gummadi, Krishna P.},
  year = {2017},
  month = apr,
  pages = {1171--1180},
  publisher = {International World Wide Web Conferences Steering Committee},
  address = {Perth Australia},
  doi = {10.1145/3038912.3052660},
  urldate = {2024-12-28},
  isbn = {978-1-4503-4913-0},
  langid = {english},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Zafar2017FDTDILCDM/Zafar et al. - 2017 - Fairness Beyond Disparate Treatment & Disparate Impact Learning Classification without Disparate Mi.pdf}
}

@inproceedings{Zafar2017PPNFC,
  title = {From {{Parity}} to {{Preference-based Notions}} of {{Fairness}} in {{Classification}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Zafar, Muhammad Bilal and Valera, Isabel and Rodriguez, Manuel and Gummadi, Krishna and Weller, Adrian},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2024-12-29},
  abstract = {The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.},
  file = {/Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Zafar2017PPNFC/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf}
}
