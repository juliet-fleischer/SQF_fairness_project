% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.2 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{nty/global//global/global}
    \entry{castelnovo2022}{article}{}
      \name{author}{6}{}{%
        {{hash=6c3ab60bce254137083c6d7874df46ed}{%
           family={Castelnovo},
           familyi={C\bibinitperiod},
           given={Alessandro},
           giveni={A\bibinitperiod}}}%
        {{hash=a2c7ac5ae0109270013f7cbf8cc88405}{%
           family={Crupi},
           familyi={C\bibinitperiod},
           given={Riccardo},
           giveni={R\bibinitperiod}}}%
        {{hash=4db8aa9f51517be9fd795e6356b71550}{%
           family={Greco},
           familyi={G\bibinitperiod},
           given={Greta},
           giveni={G\bibinitperiod}}}%
        {{hash=61e621245c669c73fbaa0363f74bc2df}{%
           family={Regoli},
           familyi={R\bibinitperiod},
           given={Daniele},
           giveni={D\bibinitperiod}}}%
        {{hash=2ddef8f8c0b16e1c67d2d2a34de1f878}{%
           family={Penco},
           familyi={P\bibinitperiod},
           given={Ilaria\bibnamedelima Giuseppina},
           giveni={I\bibinitperiod\bibinitdelim G\bibinitperiod}}}%
        {{hash=b8033a196d6f40ea8138545d3d46fcc1}{%
           family={Cosentini},
           familyi={C\bibinitperiod},
           given={Andrea\bibnamedelima Claudio},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{fullhash}{da8eb7772746823d2c9b597e7cd92623}
      \strng{bibnamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authorbibnamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authornamehash}{0d3312aef79e31263c1a2b638623053a}
      \strng{authorfullhash}{da8eb7772746823d2c9b597e7cd92623}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Abstract In recent years, the problem of addressing fairness in machine learning (ML) and automatic decision making has attracted a lot of attention in the scientific communities dealing with artificial intelligence. A plethora of different definitions of fairness in ML have been proposed, that consider different notions of what is a ``fair decision'' in situations impacting individuals in the population. The precise differences, implications and ``orthogonality'' between these notions have not yet been fully analyzed in the literature. In this work, we try to make some order out of this zoo of definitions.}
      \field{issn}{2045-2322}
      \field{journaltitle}{Scientific Reports}
      \field{langid}{english}
      \field{month}{3}
      \field{number}{1}
      \field{title}{A Clarification of the Nuances in the Fairness Metrics Landscape}
      \field{urlday}{23}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{12}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{4209}
      \range{pages}{1}
      \verb{doi}
      \verb 10.1038/s41598-022-07939-1
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Castelnovo2022cnfmlb/Castelnovo et al. - 2022 - A clarification of the nuances in the fairness metrics landscape.pdf
      \endverb
    \endentry
    \entry{caton2024}{article}{}
      \name{author}{2}{}{%
        {{hash=1e2b4a1204b7e8f7c7d379a22d93c699}{%
           family={Caton},
           familyi={C\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
        {{hash=df5dadd2d3fbb65e0e1641bab398afcd}{%
           family={Haas},
           familyi={H\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
      }
      \strng{namehash}{5533b0453780985b76fdb63a216864f2}
      \strng{fullhash}{5533b0453780985b76fdb63a216864f2}
      \strng{bibnamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authorbibnamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authornamehash}{5533b0453780985b76fdb63a216864f2}
      \strng{authorfullhash}{5533b0453780985b76fdb63a216864f2}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{shorttitle}
      \field{abstract}{When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organizes approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarizing open challenges articulated as five dilemmas for fairness research.}
      \field{issn}{0360-0300, 1557-7341}
      \field{journaltitle}{ACM Computing Surveys}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{7}
      \field{shorttitle}{Fairness in {{Machine Learning}}}
      \field{title}{Fairness in {{Machine Learning}}: {{A Survey}}}
      \field{urlday}{23}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{56}
      \field{year}{2024}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 38}
      \range{pages}{38}
      \verb{doi}
      \verb 10.1145/3616865
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Caton2024FMLSb/Caton und Haas - 2024 - Fairness in Machine Learning A Survey.pdf
      \endverb
    \endentry
    \entry{corbett-davies}{article}{}
      \name{author}{5}{}{%
        {{hash=341a17b1faf8ad74034822ff2f891742}{%
           family={{Corbett-Davies}},
           familyi={C\bibinitperiod},
           given={Sam},
           giveni={S\bibinitperiod}}}%
        {{hash=dc87d955c84b91ba3d881af1c1a93666}{%
           family={Gaebler},
           familyi={G\bibinitperiod},
           given={Johann\bibnamedelima D},
           giveni={J\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
        {{hash=a4b30e51359854c0d89aa15a6279a424}{%
           family={Nilforoshan},
           familyi={N\bibinitperiod},
           given={Hamed},
           giveni={H\bibinitperiod}}}%
        {{hash=85bd24ef7b2a76727f6692bb1bd9d048}{%
           family={Shroff},
           familyi={S\bibinitperiod},
           given={Ravi},
           giveni={R\bibinitperiod}}}%
        {{hash=769c0f637707fcc5e9ebb71ce29eb5d1}{%
           family={Goel},
           familyi={G\bibinitperiod},
           given={Sharad},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{fullhash}{4544570fa35bc719cb1465901595a433}
      \strng{bibnamehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{authorbibnamehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{authornamehash}{5a8687cd6a4a9ab3df98cc3620677781}
      \strng{authorfullhash}{4544570fa35bc719cb1465901595a433}
      \field{sortinit}{C}
      \field{sortinithash}{4d103a86280481745c9c897c925753c0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The field of fair machine learning aims to ensure that decisions guided by algorithms are equitable. Over the last decade, several formal, mathematical definitions of fairness have gained prominence. Here we first assemble and categorize these definitions into two broad families: (1) those that constrain the effects of decisions on disparities; and (2) those that constrain the effects of legally protected characteristics, like race and gender, on decisions. We then show, analytically and empirically, that both families of definitions typically result in strongly Pareto dominated decision policies. For example, in the case of college admissions, adhering to popular formal conceptions of fairness would simultaneously result in lower student-body diversity and a less academically prepared class, relative to what one could achieve by explicitly tailoring admissions policies to achieve desired outcomes. In this sense, requiring that these fairness definitions hold can, perversely, harm the very groups they were designed to protect. In contrast to axiomatic notions of fairness, we argue that the equitable design of algorithms requires grappling with their context-specific consequences, akin to the equitable design of policy. We conclude by listing several open challenges in fair machine learning and offering strategies to ensure algorithms are better aligned with policy goals.}
      \field{langid}{english}
      \field{title}{The {{Measure}} and {{Mismeasure}} of {{Fairness}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Corbett-DaviesMMFa/Corbett-Davies et al. - The Measure and Mismeasure of Fairness.pdf
      \endverb
    \endentry
    \entry{dwork2012}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=c6d8866083f4c03af5e37e032df87089}{%
           family={Dwork},
           familyi={D\bibinitperiod},
           given={Cynthia},
           giveni={C\bibinitperiod}}}%
        {{hash=e771760b6d0d33f8b4dfd907d8d57ac2}{%
           family={Hardt},
           familyi={H\bibinitperiod},
           given={Moritz},
           giveni={M\bibinitperiod}}}%
        {{hash=1813007d96ae80284b22fd39a7996df9}{%
           family={Pitassi},
           familyi={P\bibinitperiod},
           given={Toniann},
           giveni={T\bibinitperiod}}}%
        {{hash=6ff631f4e02261486e4a7d47b7aa7d0b}{%
           family={Reingold},
           familyi={R\bibinitperiod},
           given={Omer},
           giveni={O\bibinitperiod}}}%
        {{hash=77a44166b8233ae29b20f292779447e0}{%
           family={Zemel},
           familyi={Z\bibinitperiod},
           given={Richard},
           giveni={R\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Cambridge Massachusetts}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{f676b30234623eef69e5ed6be9df75b0}
      \strng{fullhash}{da7e8d8f8286359debe7cd945feb7bf5}
      \strng{bibnamehash}{f676b30234623eef69e5ed6be9df75b0}
      \strng{authorbibnamehash}{f676b30234623eef69e5ed6be9df75b0}
      \strng{authornamehash}{f676b30234623eef69e5ed6be9df75b0}
      \strng{authorfullhash}{da7e8d8f8286359debe7cd945feb7bf5}
      \field{sortinit}{D}
      \field{sortinithash}{6f385f66841fb5e82009dc833c761848}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We study fairness in classification, where individuals are classified, e.g., admitted to a university, and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier (the university). The main conceptual contribution of this paper is a framework for fair classification comprising (1) a (hypothetical) task-specific metric for determining the degree to which individuals are similar with respect to the classification task at hand; (2) an algorithm for maximizing utility subject to the fairness constraint, that similar individuals are treated similarly. We also present an adaptation of our approach to achieve the complementary goal of ``fair affirmative action,'' which guarantees statistical parity (i.e., the demographics of the set of individuals receiving any classification are the same as the demographics of the underlying population), while treating similar individuals as similarly as possible. Finally, we discuss the relationship of fairness to privacy: when fairness implies privacy, and how tools developed in the context of differential privacy may be applied to fairness.}
      \field{booktitle}{Proceedings of the 3rd {{Innovations}} in {{Theoretical Computer Science Conference}}}
      \field{isbn}{978-1-4503-1115-1}
      \field{langid}{english}
      \field{month}{1}
      \field{title}{Fairness through Awareness}
      \field{urlday}{29}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{year}{2012}
      \field{urldateera}{ce}
      \field{pages}{214\bibrangedash 226}
      \range{pages}{13}
      \verb{doi}
      \verb 10.1145/2090236.2090255
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Dwork2012Faa/Dwork et al. - 2012 - Fairness through awareness.pdf
      \endverb
    \endentry
    \entry{kusner}{article}{}
      \name{author}{4}{}{%
        {{hash=37d0d7a3ba0cb7a7253c7b690ce4e8c6}{%
           family={Kusner},
           familyi={K\bibinitperiod},
           given={Matt\bibnamedelima J},
           giveni={M\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
        {{hash=b072a1ed2f750bfeecb0466a94cc478f}{%
           family={Loftus},
           familyi={L\bibinitperiod},
           given={Joshua},
           giveni={J\bibinitperiod}}}%
        {{hash=c40407f768dd3e8b296a11750ad77f41}{%
           family={Russell},
           familyi={R\bibinitperiod},
           given={Chris},
           giveni={C\bibinitperiod}}}%
        {{hash=4047ee6235adca502c24f37317e79d8e}{%
           family={Silva},
           familyi={S\bibinitperiod},
           given={Ricardo},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{fullhash}{31480a6997ada758f14a82daae7e9c2d}
      \strng{bibnamehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{authorbibnamehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{authornamehash}{5ed8ad78ef3e89e7f60ebcdfc729fd12}
      \strng{authorfullhash}{31480a6997ada758f14a82daae7e9c2d}
      \field{sortinit}{K}
      \field{sortinithash}{c02bf6bff1c488450c352b40f5d853ab}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.}
      \field{langid}{english}
      \field{title}{Counterfactual {{Fairness}}}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/KusnerCF/Kusner et al. - Counterfactual Fairness.pdf
      \endverb
    \endentry
    \entry{mehrabi2022}{article}{}
      \name{author}{5}{}{%
        {{hash=e49a830517defa430474d9c2b5c22d83}{%
           family={Mehrabi},
           familyi={M\bibinitperiod},
           given={Ninareh},
           giveni={N\bibinitperiod}}}%
        {{hash=6fc2e21e9c786462d54f7e2eeae61c19}{%
           family={Morstatter},
           familyi={M\bibinitperiod},
           given={Fred},
           giveni={F\bibinitperiod}}}%
        {{hash=125158a659e4885af8901c5155a4e57a}{%
           family={Saxena},
           familyi={S\bibinitperiod},
           given={Nripsuta},
           giveni={N\bibinitperiod}}}%
        {{hash=8aea0a421db201a447f5fec22e73a92f}{%
           family={Lerman},
           familyi={L\bibinitperiod},
           given={Kristina},
           giveni={K\bibinitperiod}}}%
        {{hash=73b781905368dfe2cc66cef5e33e0d32}{%
           family={Galstyan},
           familyi={G\bibinitperiod},
           given={Aram},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{fullhash}{70df8b815290d4668300a842f0dfe8bc}
      \strng{bibnamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authorbibnamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authornamehash}{cc037d6c925e3502b4bada6900e5b052}
      \strng{authorfullhash}{70df8b815290d4668300a842f0dfe8bc}
      \field{sortinit}{M}
      \field{sortinithash}{4625c616857f13d17ce56f7d4f97d451}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.}
      \field{issn}{0360-0300, 1557-7341}
      \field{journaltitle}{ACM Computing Surveys}
      \field{langid}{english}
      \field{month}{7}
      \field{number}{6}
      \field{title}{A {{Survey}} on {{Bias}} and {{Fairness}} in {{Machine Learning}}}
      \field{urlday}{7}
      \field{urlmonth}{1}
      \field{urlyear}{2025}
      \field{volume}{54}
      \field{year}{2022}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 35}
      \range{pages}{35}
      \verb{doi}
      \verb 10.1145/3457607
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/mehrabi2022/Mehrabi et al. - 2022 - A Survey on Bias and Fairness in Machine Learning.pdf
      \endverb
    \endentry
    \entry{verma2018}{inproceedings}{}
      \name{author}{2}{}{%
        {{hash=83905e77def6a8cd568f21d89ce43fc8}{%
           family={Verma},
           familyi={V\bibinitperiod},
           given={Sahil},
           giveni={S\bibinitperiod}}}%
        {{hash=e668b98b45e06d4af2230904d870f6e8}{%
           family={Rubin},
           familyi={R\bibinitperiod},
           given={Julia},
           giveni={J\bibinitperiod}}}%
      }
      \list{location}{1}{%
        {Gothenburg Sweden}%
      }
      \list{publisher}{1}{%
        {ACM}%
      }
      \strng{namehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{fullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{bibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorbibnamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authornamehash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \strng{authorfullhash}{d2d267df27eeb3b8c2bbbf8ae4b7ef30}
      \field{sortinit}{V}
      \field{sortinithash}{afb52128e5b4dc4b843768c0113d673b}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.}
      \field{booktitle}{Proceedings of the {{International Workshop}} on {{Software Fairness}}}
      \field{isbn}{978-1-4503-5746-3}
      \field{langid}{english}
      \field{month}{5}
      \field{title}{Fairness Definitions Explained}
      \field{urlday}{16}
      \field{urlmonth}{11}
      \field{urlyear}{2024}
      \field{year}{2018}
      \field{urldateera}{ce}
      \field{pages}{1\bibrangedash 7}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1145/3194770.3194776
      \endverb
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Verma2018Fde/Verma und Rubin - 2018 - Fairness definitions explained.pdf
      \endverb
    \endentry
    \entry{Zafar2017PPNFC}{inproceedings}{}
      \name{author}{5}{}{%
        {{hash=651ed744d2d757bbdb4ef8519fdb9745}{%
           family={Zafar},
           familyi={Z\bibinitperiod},
           given={Muhammad\bibnamedelima Bilal},
           giveni={M\bibinitperiod\bibinitdelim B\bibinitperiod}}}%
        {{hash=86077c699c3bcb4f9fecce395f1e6f41}{%
           family={Valera},
           familyi={V\bibinitperiod},
           given={Isabel},
           giveni={I\bibinitperiod}}}%
        {{hash=a189b5a8e9f39b81e8445433ec383ff6}{%
           family={Rodriguez},
           familyi={R\bibinitperiod},
           given={Manuel},
           giveni={M\bibinitperiod}}}%
        {{hash=0d02e39696f2420a11d3ee706088466b}{%
           family={Gummadi},
           familyi={G\bibinitperiod},
           given={Krishna},
           giveni={K\bibinitperiod}}}%
        {{hash=89be134d6b955c508c4267860047bdd9}{%
           family={Weller},
           familyi={W\bibinitperiod},
           given={Adrian},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Curran Associates, Inc.}%
      }
      \strng{namehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{fullhash}{8a150b1f890c166a574abb59887dff31}
      \strng{bibnamehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{authorbibnamehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{authornamehash}{5a3b59ce7c7b687c4ebf7f069fab18c2}
      \strng{authorfullhash}{8a150b1f890c166a574abb59887dff31}
      \field{sortinit}{Z}
      \field{sortinithash}{96892c0b0a36bb8557c40c49813d48b3}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness -- given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.}
      \field{booktitle}{Advances in {{Neural Information Processing Systems}}}
      \field{title}{From {{Parity}} to {{Preference-based Notions}} of {{Fairness}} in {{Classification}}}
      \field{urlday}{29}
      \field{urlmonth}{12}
      \field{urlyear}{2024}
      \field{volume}{30}
      \field{year}{2017}
      \field{urldateera}{ce}
      \verb{file}
      \verb /Users/julietfleischer/Library/Mobile Documents/iCloud~md~obsidian/Documents/Studium/PDFs/Zafar2017PPNFC/Zafar et al. - 2017 - From Parity to Preference-based Notions of Fairness in Classification.pdf
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

